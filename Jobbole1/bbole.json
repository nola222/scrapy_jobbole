{"art_url": "http://python.jobbole.com/88892/", "create_time": "2017-11-19", "art_content": ["通俗的讲就是对曲线进行采样简化，即在曲线上取有限个点，将其变为折线，并且能够在一定程度保持原有形状。比较常用的两种抽稀算法是：道格拉斯-普克(Douglas-Peuker)算法和垂距限值法。", "Douglas-Peuker算法(DP算法)过程如下:", "这种算法的抽稀精度与阈值有很大关系，阈值越大，简化程度越大，点减少的越多；反之简化程度越低，点保留的越多，形状也越趋于原曲线。", "下面是Python代码实现:", "垂距限值法其实和DP算法原理一样，但是垂距限值不是从整体角度考虑，而是依次扫描每一个点，检查是否符合要求。", "算法过程如下:", "下面是Python代码实现：", "其实DP算法和垂距限值法原理一样，DP算法是从整体上考虑一条完整的曲线，实现时较垂距限值法复杂，但垂距限值法可能会在某些情况下导致局部最优。另外在实际使用中发现采用点到另外两点所在直线距离的方法来判断偏离，在曲线弧度比较大的情况下比较准确。如果在曲线弧度比较小，弯曲程度不明显时，这种方法抽稀效果不是很理想，建议使用三点所围成的三角形面积作为判断标准。下面是抽稀效果:", "\n", " "], "art_img_url": "http://jbcdn2.b0.upaiyun.com/2017/11/3be8f2c50649ea9862264793b74bdc66.png", "art_title": "曲线点抽稀算法- Python 实现"}
{"create_time": "2018-02-04", "art_content": ["中介绍了计算图以及前向传播的实现，本文中将主要介绍对于模型优化非常重要的反向传播算法以及反向传播算法中梯度计算的实现。因为在计算梯度的时候需要涉及到矩阵梯度的计算，本文针对几种常用操作的梯度计算和实现进行了较为详细的介绍。如有错误欢迎指出。", "首先先简单总结一下, 实现反向传播过程主要就是完成两个任务:", "再附上SimpleFlow的代码地址: ", "对于我们构建的模型进行优化通常需要两步：1.求损失函数针对变量的梯度；2.根据梯度信息进行参数优化(例如梯度下降). 那么该如何使用我们构建的计算图来计算损失函数对于图中其他节点的梯度呢？通过", "。我们还是通过上篇中的表达式", "对应的计算图来说明:", "我们把上面的操作节点使用字母进行标记，可以将每个操作看成一个函数，接受一个或两个输入有一个或者多个输出, 则上面的表达", "那么根据链式法则我们可以得到", "对", "的导数为:"], "art_url": "http://python.jobbole.com/89012/", "art_img_url": "http://pytlab.org/assets/images/blog_img/2018-01-24-%E5%AE%9E%E7%8E%B0%E5%B1%9E%E4%BA%8E%E8%87%AA%E5%B7%B1%E7%9A%84TensorFlow-%E4%B8%80-%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%B8%8E%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD/feature.png", "art_title": "实现属于自己的TensorFlow(二) - 梯度计算与反向传播"}
{"create_time": "2017-12-14", "art_content": ["程序员都自视清高，觉得自己是创造者，经常鄙视不太懂技术的产品或者QA。可悲的是，程序员之间也相互鄙视，", "流传甚广，作为一个Python程序员，自然最关心的是下面这幅图啦", "我们项目组一值使用Python2.7，虽然我们也知道Python3的诸多好处，也曾经蠢蠢欲动过，但由于各种历史原因，以及业务的压力，我们只可能继续使用Python2.7。更悲哀的是，我们组不是那么international，所以代码中还是涉及到大量的中文，因此偶尔也会遇到乱码以及UnicodeError，于是生活在了鄙视链的末端。", "因此，本文的目标是解释清楚python2.7中unicode、str的编解码关系，力求在鄙视链中前进一步。", "：本文实验主要基于win7，Python2.7；以及Linux ，Python2.7。除非特殊说明，所有的命令都是在终端中交互式输入；如果没有强调平台，那么就是window上的结果。下面是一些默认的环境信息（其重要性后文会介绍）", "windows", "注意，上面", "，在", "可以查看。", "Linux", "首先来说一说gbk gb2312 unicode utf-8这些术语，这些术语与语言无关。", "计算机的世界只有0和1，因此任何字符（也就是实际的文字符号）也是由01串组成。计算机为了运算方便，都是8个bit组成一个字节（Byte），字符表达的最小单位就是字节，即一个字符占用一个或者多个字节。字符编码（", "g）就是字集码，编码就是将字符集中的字符映射为一个唯一二进制的过程。", "计算机发源于美国，使用的是英文字母（字符），所有26个字母的大小写加上数字0到10，加上符号和控制字符，总数也不多，用一个字节（8个bit）就能表示所有的字符，这就是ANSI的“Ascii”编码（American Standard Code for Information Interchange，美国信息互换标准代码）。比如，小写字母‘a’的ascii 码是01100001，换算成十进制就是97，十六进制就是0x61。计算机中，一般都是用十六进制来描述字符编码。", "但是当计算机传到中国的时候，ASCII编码就行不通了，汉字这么多，一个字节肯定表示不下啊，于是有了GB 2312（中国国家标准简体中文字符集）。GB2312使用两个字节来对一个字符进行编码，其中前面的一个字节（称之为高字节）从0xA1用到 0xF7，后面一个字节（低字节）从0xA1到0xFE，GB2312能表示几千个汉字，而且与asill吗也是兼容的。", "但后来发现，GB2312还是不够用，于是进行扩展，产生了GBK（即汉字内码扩展规范）， GBK同Gb2312一样，两个字节表示一个字符，但区别在于，放宽了对低字节的要求，因此能表示的范围扩大到了20000多。后来，为了容纳少数名族，以及其他汉字国家的文字，出现了GB13080。GB13080是兼容GBK与GB2312的，能容纳更多的字符，与GBK与GB2312不同的是，GB18030采用单字节、双字节和四字节三种方式对字符编码", "因此，就我们关心的汉字而言，三种编码方式的表示范围是：", "GB18030 》 GBK 》 GB2312", "即", "。后面也会看到，一个汉字可以用GBK表示，但不一定能被GB2312所表示", "当然，世界上还有更多的语言与文字，每种文字都有自己的一套编码规则，这样一旦跨国就会出现乱码，亟待一个全球统一的解决办法。这个时候ISO（国际标准化组织）出马了，发明了”Universal Multiple-Octet Coded Character Set”，简称 UCS, 俗称 “unicode”。目标很简单：废了所有的地区性编码方案，重新搞一个包括了地球上所有文化、所有字母和符号 的编码！", "unicode每种语言中的每个字符设定了统一并且唯一的二进制编码，以满足跨语言、跨平台进行文本转换、处理的要求。unicode编码一定以u开头。", "但是，unicode只是一个编码规范，是所有字符对应二进制的集合，而不是具体的编码规则。或者说，", "。这个就跟GBK这些不一样，GBK是表里如下，表现形式即存储形式。", "比如汉字“严”的unicode编码是u4e25，对应的二进制是1001110 00100101，但是当其经过网络传输或者文件存储时，是没法知道怎么解析这些二进制的，容易和其他字节混在一起。那么怎么存储unicode呢，于是出现了UTF（UCS Transfer Format），这个是具体的编码规则，即UTF的表现形式与存储格式是一样的。", "因此，可以说，", "。只不过，转换成Utf-8，大家都能懂，更懂用，而转换成GBK，只有中国人才看得懂", "UTF也有不同的实现，如UTF-8， UTF-16， 这里以UTF-8为例进行讲解（下面一小节引用了", "）。", "UTF-8最大的一个特点，就是它是一种变长的编码方式。它可以使用1~4个字节表示一个符号，根据不同的符号而变化字节长度。UTF-8的编码规则很简单，只有二条：", "1）对于单字节的符号，字节的第一位设为0，后面7位为这个符号的unicode码。因此对于英语字母，UTF-8编码和ASCII码是相同的。", "2）对于n字节的符号（n>1），第一个字节的前n位都设为1，第n+1位设为0，后面字节的前两位一律设为10。剩下的没有提及的二进制位，全部为这个符号的unicode码。", "下表总结了编码规则，字母x表示可用编码的位。", "以汉字“严”为例，演示如何实现UTF-8编码。", "已知“严”的unicode是4E25（100111000100101），根据上表，可以发现4E25处在第三行的范围内（0000 0800-0000 FFFF），因此“严”的UTF-8编码需要三个字节，即格式是“1110xxxx 10xxxxxx 10xxxxxx”。然后，从“严”的最后一个二进制位开始，依次从后向前填入格式中的x，多出的位补0。这样就得到了，“严”的UTF-8编码是“11100100 10111000 10100101”，转换成十六进制就是E4B8A5。", "下面使用Python语言来验证上面的理论。在这一章节中，当提到unicode，一般是指unicode type，即Python中的类型；也会提到unicode编码、unicode函数，请大家注意区别。", "另外，对于编码，也有两种意思。第一个是名字，指的是字符的二进制表示，如unicode编码、gbk编码。第二个是动词，指的是从字符到二进制的映射过程。不过后文中，编码作为动词，狭义理解为从unicode类型转换成str类型的过程，解码则是相反的过程。", "在python2.7中，有两种“字符串”类型，分别是str 与 unicode，他们有同一个基类basestring。str是plain string，其实应该称之为", "，因为是每一个字节换一个单位长度。而unicode就是unicode string，这才是真正的", "，一个字符（可能多个字节）算一个单位长度。", "python2.7中，unicode类型需要在文本之间加u表示。", "从上可以看到，第一，us、s的类型是不一样的；其二，同一个汉字，不同的类型其长度也是不一样的，对于unicode类型的实例，其长度一定是字符的个数，而对于str类型的实例，其长度是字符对应的字节数目。这里强调一下，s（s = ‘严’）的长度在不同的环境下是不一样的！后文会解释", "这是python中两个magic method，很容易让新手迷糊，因为很多时候，二者的实现是一样的，但是这两个函数是用在不同的地方", "_str__， 主要是用于展示，", "或者", "的时候调用，返回值一定是一个str 对象", "__repr__， 是被", "， 或者在终端直接打", "的时候调用", "可以看到，不使用print返回的是一个更能反映对象本质的结果，即us是一个unicode对象（最前面的u表示，以及unicode编码是用的u），且“严”的unicode编码确实是4E25。而print调用可us.__str__，等价于print str(us)，使得结果对用户更友好。那么unicode.__str__是怎么转换成str的呢，答案会在后面揭晓", "前面已经提到，unicode只是编码规范（只是字符与二进制的映射集合），而utf-8是具体的编码规则（不仅包含字符与二进制的映射集合，而且映射后的二进制是可以用于存储和传输的），即utf-8负责把unicode转换成可存储和传输的二进制字符串即str类型，我们称这个转换过程为编码。而从str类型到unicode类型的过程，我们称之为解码。", "Python中使用decode()和encode()来进行解码和编码，以unicode类型作为中间类型。如下图所示", "。for example", "从上可以看出encode与decode两个函数的作用，也可以看出’严’的utf8编码是E4B8A5。", "就是说我们使用unicode.encode将unicode类型转换成了str类型，在上面也提到unicode.__str__也是将unicode类型转换成str类型。二者有什么却比呢", "首先看看文档", "注意：str.encode 这里的str是basestring，是str类型与unicode类型的基类", "可以看到encode方法是有可选的参数：encoding 和 errors，在上面的例子中encoding即为utf-8；而__str__是没有参数的，我们可以猜想，对于unicode类型，__str__函数一定也是使用了某种encoding来对unicode进行编码。", "首先不禁要问，如果encode方法没有带入参数，是什么样子的：", "不难看出，默认使用的就是ascii码来对unicode就行编码，为什么是ascii码，其实就是", "编码（sys.getdefaultencoding的返回值）。ascii码显然无法表示汉字，于是抛出了异常。而使用utf-8编码的时候，由于utf能够表示这个汉字，所以没报错。", "如果直接打印ss（us.encode(‘utf-8’)的返回值）会怎么样", "结果略有些奇怪，us.__str__(即直接打印us）的结果不一样，那么试试encoding = gbk呢？", "U got it! 事实上也是如此，python会采用", "的编码（用locale.getdefaultlocale()查看，windows是为gbk）将unicode编码成str类型。", "在Linux（终端编码为utf-8），结果如下：", "注意上面的乱码！", "在上上小节，介绍了unicode可以通过utf-8编码（encoding = utf-8），转换成utf-8表示的str，在上一节也可以看出unicode也可以通过gbk编码（encoding=gbk），转换成gbk表示的str。这里有点晕，留作第一个问题，后面解释", "unicode与utf8之间的相互转换可以计算得知，但unicode与gbk之间的相互转换没有计算公式，就只能靠查表了，就是说有一张映射表，有某一个汉字对应的unicode表示与gbk表示的映射关系", "从上不难看出，严的unicdoe编码是4e25，GBK编码是d1cf，因此us通过gbk编码就是d1cf。同样也能看到，GB18030，GBK，GB2312是兼容的", "， ss是一个str类型，直接打印结果有点奇怪，一个“涓”字，那一个str类型的“涓”是哪些二进制组成的呢", "可以看到，str类型的“涓”，其二进制是E4B8，跟’严’的utf8编码（E4B8A5）相差了一个A5，那么就是因为A5显示不出来，验证如下：", "因此，只是碰巧显示了“涓”而已，事实上ss跟“”涓“”毫无关系", "在上上小节，提到了utf-8编码的str，与gbk编码的str，感觉有点绕。我们知道，一个汉字‘严’，可存储的编码格式可以是gbk（’xd1xcf’），也可以是utf-8（’xe4xb8xa5’），那么当我们在终端敲入这个汉字的时候，是哪一种格式呢？取决于", "windows上（默认终端编码为gbk）：", "Linux上（默认终端编码为utf-8）：", "同样一个汉字，同样都是Python中的str类型，在不同的编码格式下，其二进制是不一样的。因此，其长度也是不一样的，对于str类型，其长度是对应的字节长度。", "也能看出gbk编码的字节长度一般小于utf-8，这也是gbk继续存在的一个原因。", "这里，", "！这个也不难理解。", "str类型到unicode类型的转换，出了上面提到的str.decode，还有一个unicode函数。两个函数的签名为：", "二者参数相同，事实上二者是等价的，encoding的默认值也是一样的，都是sys.getdefaultencoding()的结果。for example：", "第一个UnicodeDecodeError，就是因为系统默认的编码是asill吗；第二个UnicodeDecodeError，是因为，s（str类型的实例）的编码取决于终端默认编码（即windows下的gbk），为了能打印出来，也就必须用gbk编码来表示这个str，因此只能查询gbk与unicode的映射表将s转换成unicode类型。", "在诸多Python代码中，都会看到这么一段：", "不难猜想，", "跟", "是配对的，为啥要将系统的默认编码设置成utf-8，其实就是解决str到unicode的转换问题。", "上一小节已经提到过，使用unicode函数将str类型转换成unicode类型时，要考虑两个因素：第一，str本身是什么编码的；第二，如果没有传入encoding参数，默认使用sys.getdefaultencoding。encoding参数必须与str本身的编码对应，否则就是UnicodeDecodeError。", "写python代码的程序都知道，我们要在py文件第一行写上：", "这句话的作用在于，告诉编辑器，该文件里面的所有str都采用utf-8编码，且存储文件的时候也是使用utf-8格式。", "然后文件中就会使用下面的这种代码。", "使用unicode强制转换的时候，都不习惯带参数，为了保证encoding参数必须与str本身的编码一致，所以使用", "将系统默认编码设置为utf-8", "下面介绍几种常见的乱码与异常UnicodeError， 大多数乱码或者异常的原因在前面已经讲过了，同时，对于一些乱码，也试图给出可行的解决办法。", "UnicodeError包括UnicodeDecodeError 与UnicodeEncodeError ，前者是decode也就是str转unicode的时候出了异常，后者则是encode也就是unicode转str的时候出了异常。", "例子就是上面反复提到的例子", "如果一个str类型来自网络或者文件读取，最好先按照对端encode的方式先decode成unicode，然后再输出（输出的时候会自动转换成期望终端支持的编码格式的str）", "直接上例子", "可以看到，‘囍’字可以被gbk编码，但是不能被gb2312编码。", "在上面讲unicode函数的时候已经举过例子，会爆出UnicodeDecodeError 异常。", "这个错误比较的原因，更多来自str到unicode的默认转换，比如一个str与一个unicode相加的时候：", "unicode 与 str相加，str会转换为unicode,使用默认的unicode(strobj, encoding = sys.getdefaultencoding())", "某些情况下，我们打印出一个str类型，看到结果是’\\u4e25’， 或者’u4e25’，对于这个字符串，是不是很眼熟，不错， ‘严‘的unicode编码就是u’u4e25’。仔细一看，只是在引号前面多了一个u（表示是一个unicode类型）。那么当我们看到一个’u4e25’的时候，怎么知道对应的汉字是什么？对于已知的这种格式的str，自然可以手动加一个u，然后在终端输出，但是如果是一个变量，需要自动转换成unicode呢，这个时候就可以使用", "中的unicode_escape", "有时候，也会看到类似这样的str，’\\xd1\\xcf’， 看起来也很熟悉，跟汉字“严”的gbk编码’xd1xcf’很像，区别在于前者多了一个‘’， 这样就无法解释成一个十六进制了。解决办法是", "中的string_escape", "在这里留下一个问题：", "返回值是True 还是 False呢？当然这里故意省去了上下文环境，不过明确的说，在不同的编码环境下，答案是不一样的，原因都在上文中！", "不管怎么样解释，python2.x中的字符编码还是一件让人头疼的事情，即使搞懂了，之后遇到了也可能忘记。对于这个问题，诸多建议如下：", "第一：使用python3，就不用再纠结str于unicode了；但是这个很难开发者说了算；", "第二：不要使用中文，注释什么的都用英文；理想很丰满，现实很难，只是导致大量的拼音；", "第三：对于中文字符串，不要用str表示，而是用unicode表示；现实中也不好实施，大家都不愿意多写一个u", "第四：只在传输，或者持久化的时候对unicode进行encode，相反的过程时decode", "第五：对于网络接口，约定好编解码格式，强烈建议使用utf-8", "第六：看到UnicodeXXXError不要慌，如果XXX是Encode，那么一定是unicode转str的时候出了问题；如果是Decode，一定是str转unicode的时候出了问题。"], "art_url": "http://python.jobbole.com/88967/", "art_img_url": "http://jbcdn2.b0.upaiyun.com/2017/11/8ef4df4888b257b5ea7bbd4b033a519c.png", "art_title": "不想再被鄙视？那就看进来！ 一文搞懂Python2字符编码"}
{"create_time": "2017-11-27", "art_content": ["在Python中，属性查找（attribute lookup）是比较复杂的，特别是涉及到描述符descriptor的时候。", "在", "末尾，给出了一段代码，就涉及到descriptor与attribute lookup的问题。而get系列函数(__get__, __getattr__, __getattribute__) 也很容易搞晕，本文就这些问题简单总结一下。", "首先，我们知道：", "按照python doc，如果obj是某个类的实例，那么obj.name（以及等价的getattr(obj,’name’)）首先调用__getattribute__。如果类定义了__getattr__方法，那么在__getattribute__抛出 AttributeError 的时候就会调用到__getattr__，而对于描述符(__get__）的调用，则是发生在__getattribute__内部的。官网文档是这么描述的", "obj = Clz(), 那么obj.attr 顺序如下：", "（1）如果“attr”是出现在Clz或其基类的__dict__中， 且attr是data descriptor， 那么调用其__get__方法, 否则", "（2）如果“attr”出现在obj的__dict__中， 那么直接返回 obj.__dict__[‘attr’]， 否则", "（3）如果“attr”出现在Clz或其基类的__dict__中", "（3.1）如果attr是non-data descriptor，那么调用其__get__方法， 否则", "（3.2）返回 __dict__[‘attr’]", "（4）如果Clz有__getattr__方法，调用__getattr__方法，否则", "（5）抛出AttributeError", "下面是测试代码：", "注意第50行，change_attr给实例的__dict__里面增加了两个属性。通过上下两条print的输出如下：", "调用change_attr方法之后，dd_base既出现在类的__dict__（作为data descriptor）, 也出现在实例的__dict__， 因为attribute lookup的循序，所以优先返回的还是Clz.__dict__[‘dd_base’]。而ndd_base虽然出现在类的__dict__， 但是因为是nondata descriptor，所以优先返回obj.__dict__[‘dd_base’]。其他：line48,line56表明了__getattr__的作用。line49表明obj.__dict__优先于Clz.__dict__", "我们再来看看", "的这段代码。", "cached_property是一个non-data descriptor。在TestClz中，用cached_property装饰方法complex_calc，返回值是一个descriptor实例，所以在调用的时候没有使用小括号。", "第一次调用t.complex_calc之前，obj(t)的__dict__中没有”complex_calc“， 根据查找顺序第三条，执行cached_property.__get__, 这个函数代用缓存的complex_calc函数计算出结果，并且把结果放入obj.__dict__。那么第二次访问t.complex_calc的时候，根据查找顺序，第二条有限于第三条，所以就直接返回obj.__dict__[‘complex_calc’]。bottle的源码中还有两个descriptor，非常厉害！", "前面提到过，类的也是对象，类是元类（metaclass）的实例，所以类属性的查找顺序基本同上。区别在于第二步，由于Clz可能有基类，所以是在Clz及其基类的__dict__", "“attr，注意这里的查找并不是直接返回clz.__dict__[‘attr’]。具体来说，这第二步分为以下两种情况：", "（2.1）如果clz.__dict__[‘attr’]是一个descriptor（不管是data descriptor还是non-data descriptor），都调用其__get__方法", "（2.2）否则返回clz.__dict__[‘attr’]", "这就解释了一个很有意思的问题：method与function的问题", "Widget是一个之定义了一个func函数的类，func是类的属性，这个也可以通过Widget.__dict__、w.__dict__看到。Widget.__dict__[‘func’]返回的是一个function，但Widget.func是一个unbound method，即Widget.func并不等同于Widget.__dict__[‘func’]，按照前面的类属性的访问顺序，我们可以怀疑，func是一个descriptor，这样才不会走到第2.2这种情况。验证如下：", "Python的属性赋值（attribute assignment）也会受到descriptor（data descriptor）的影响，同时也会受到__setattr__函数的影响。当然Python中还有一个setattr，setattr(x, ‘foobar’, 123)等价于x.foobar = 123，二者都叫attribute assignment。", "首先看看__setattr__:", "那什么是normal mechanism，简单来说就是x.__dict__[‘foobar’] = 123，不管’foobar’之前是否是x的属性（当然赋值之后就一定是了）。但是如果‘’foobar‘’是类属性，且是data descriptor，那么回优先调用__set__。我们来看一个例子：", "输出如下：", "可以看到，即使Widget的实例也有一个‘a’属性，但是调用w.a的时候会调用类属性‘a’（一个descriptor）的__set__方法。如果不注释掉第18到第20行，输出如下", "可以看到，优先调用Widget 的__setattr__方法。因此：对于属性赋值，obj = Clz(), 那么obj.attr = var，按照这样的顺序："], "art_url": "http://python.jobbole.com/88937/", "art_img_url": "http://jbcdn2.b0.upaiyun.com/2015/02/edecceebedd0d04aa17bccba430ddcaf.jpg", "art_title": "深入理解 Python 的属性查找"}
{"create_time": "2017-12-26", "art_content": ["目前机器学习红遍全球。男女老少都在学机器学习模型，分类器，神经网络和吴恩达。你也想成为一份子，但你该如何开始？", "在这篇文章中我们会讲Python的重要特征和它适用于机器学习的原因，介绍一些重要的机器学习包，以及其他你可以获取更详细资源的地方。", "Python很适合用于机器学习。首先，它很简单。如果你完全不熟悉Python但是有一些其他的编程经验（C或者其他编程语言），要上手是很快的。其次，Python的社区很强大。这使得Python的文档不仅条理性好，而且容易读。你还可以在StackOverFlow上找到关于很多问题详细解答（学习基石）。再次，一个强大的社区带来的副产品就是大量有用程序库（Python内部自带的和第三方软件），基本上可以解决你所有的问题（包括机器学习）。", "Python是很慢。它不是执行最快的语言，拥有那么多好用的抽象是要付出代价的。", "但这是个可以解决的问题：程序库可以把计算量繁重的部分外包给其他更高效（但更难使用）的语言，例如C和C++。比如NumPy这个提供数值运算的程序库，就是用C写的，运行速度超快。在实际运用中，几乎所有程序库都会使用NumPy去完成计算繁重的部分。如果你看到Numpy，你应该想到它很快。", "所以你是可以让程序的运行速度跟它的低层语言实现的运行速度相比拟的。你没有必要担心程序的运行速度。", " ", "你刚开始学机器学习吗？如果你需要一个涵盖了特征工程，模型训练和模型测试所有功能的程序库，", "是你的最佳选择！这个优秀的免费软件提供了机器学习和数据挖掘所需要的所有工具。它是目前Python机器学习的标准库。要使用任何成熟的机器学习算法都推荐使用这个库。", "这个程序库支持分类和回归，实现了基本所有的经典算法（支持向量机，随机森林，朴素贝叶斯等等）。程序库的设计让迁移算法十分容易，使用不同的算法做实验非常轻松。这些经典算法可用性很强，能用于大量不同的情况。", "但这并不是Scikit-learn的全部功能，它同样可以用来做降维，聚类等等任何你所能想到的。由于它构建在Numpy和Scipy之上（所有的数值计算都是由C语言来完成的），它的运行速度也超快。", "可以告诉你这个库的功能，如果你想学习如何使用它，可以阅读", "不算是一个机器学习的程序库，但它是做自然语言处理（NLP）必须的一个库。除了用于文字处理的功能，例如聚类，分词，词干提取，标记，解析等，它还包含了大量的数据集和其他关于词法的资源（可用于模型训练）。", "把所有这些打包在一起的好处就不用再多说了。如果你对NLP感兴趣，可以看看", "!", "被广泛应用于工业界和学术界，它是所有深度学习架构的鼻祖。Theano是用Python，结合Numpy实现的。你可以用它来构建用多维数组实现神经网络。Theano会处理所有数学计算，你不需要知道底层的数学公式实现。", "早在支持使用GPU进行计算不像今天这样普及的时候，Theano就已经提供了对GPU计算的支持。这个程序库目前已经非常成熟，能够支持很多不同类型的操作。这使得Theano可以在和其他库比较的时候胜出。", "目前关于Theano最大的问题是API不是很好用，对于新手来说上手困难。不过市面上已经有了解决这个问题的封装包，比如", ", ", " 和 ", "，都可以简化Theano的使用。", "谷歌大脑团队为了内部使用创造了", "，2015年将其开源化。设计初衷是取代他们已有的封闭机器学习框架DistBelief，据说该构架太过于依赖Google的整体构架，也不够灵活，在分享代码的时候非常不方便。", "于是就有了TensorFlow。谷歌从以前的错误中吸取了教训。许多人认为TensorFlow是Theano的改进版，它提供了更灵活和好用的API。可以用于科研和工业界，同时支持使用大量的GPU进行模型训练。TensorFlow支持的操作没有Theano多，但是它的", "比Theano好。", "TensorFlow目前非常流行。如果今天这篇文章里面提到的名字你只听说了一个，那很有可能是这个。每天都有新的提到TensorFlow的博文或学术文章发表。这个流行度提供了大量的用户和", "，新人很容易上手。", "是一个提供更高层神经网络API的库，它可以基于Theano或者TensorFlow。它拥有这两个库强大的功能却又同时大大地简化了使用难度。它将用户的体验放在首要地位，提供简单的API和很有用的错误信息。", "同时Keras的设计基于模块，这就使得你能自由组合不同的模型（神经层，成本函数等等），而且模型的可扩展性很好，因为你只需要简单的将新模块跟已有的连起来即可。", "有人觉得Keras太好用了，", "。如果你开始用深度学习，可以看看", " 和 ", "，对于你可以用它做什么有个数。如果你要学习使用它，可以从 ", "开始。", "两个类似的库有", " 和 ", ", 但它们只支持Theano。如果你试过了Keras但是你不喜欢它你可以试试这些其他的库，也许它们更适合你。", "还有一个有名的深度学习架构是", "它是用Lua实现的。Facebook用Python实现了Torch，叫做", "，并将它开源了。用这个库你可以使用Torch使用的低层的库，但是你可以使用Python而不是Lua。", "PyTorch对查错的支持很好，这是因为Theano和TensorFlow使用符号计算而PyTorch则不是。使用符号计算就表明在一行代码被解释的时候，一个操作（x+y）并不会被执行，在那之前，它必须先被编译（解释成CUDA或者C语言）。这就让用Theano和TensorFlow的时候很难查错，因为很难把报错跟当前的代码联系起来。这样做有它的好处，不过查错简单不在其中。", "如果你想开始学PyTorch，", "适合初学者也会包含有难度的内容。", "你讲了这么多机器学习的包，我应该用哪一个？我怎样比较它们？我从哪里开始？", "你可以试用我们面向初学者的平台Ape Advice™，就不用烦细节的问题了。如果你完全没有接触过机器学习，从", "开始。你可以了解标记，训练和测试是怎样工作的，以及一个模型是如何被建立的。", "如果你想试试深度学习，从", "开始，毕竟这是大家公认的最简单的框架。你可以先试试，找找感觉。当你有点经验之后，你可以开始考虑你最需要的是什么：速度，不同的API，或者别的什么，之后你就能更好地决定了。", "目前有", "比较Theano，Torch和TensorFlow。没有人能说哪个最好。你要记住的是所有包都支持很多东西，而且也在不断改进，想相互比较它们也越来越难。六个月前的标准有可能已经过时了，一年前的评价说框架X没有Y功能也不一定还有效。", "最后，如果你想用NLP，可以试试", "!我们的这个平台所提供的用户界面让建造模型，训练模型和改进NLP模型都非常容易试下。你可以用事先训练好的模型处理常见问题（意见挖掘，话题探测或者提取关键字），也可以为你特有的问题设计一个新的算法。你不需要担心底层实现或者发布你的模型，我们可扩展的云系统会帮你完成这些。你可以", "，马上开始试用我们超棒的API。", "关于机器学习的网络资源很多！下面列举一些：", "这篇关于用Python库做机器学习的简介就到此为止。我想强调的是不要被细节吓住了，放手尝试。让你的好奇心指导你前进，不要害怕进行不同的实验。"], "art_url": "http://python.jobbole.com/88705/", "art_img_url": "http://wx1.sinaimg.cn/mw690/63918611gy1fmukayefezj20sw0e447u.jpg", "art_title": "使用 Python 开始机器学习"}
{"create_time": "2018-01-29", "art_content": ["人人都恨", "——那些恼人的图片，显示着你在登陆某网站前得输入的文本。设计", "的目的是，通过验证你是真实的人来避免电脑自动填充表格。但是随着深度学习和计算机视觉的兴起，现在验证码常常易被攻破。", "我拜读了 Adrian Rosebrock 写的《", "》。在书中，Adrian 描述了他是怎样用机器学习绕过纽约 E-ZPass 网站上的验证码：", "Adrian 无法接触到该应用生成", "的源代码。为了攻破该系统，他不得不下载数百张示例图片，并手动处理它们来训练他自己的系统。", "但是如果我们想攻破的是一个开源", "系统，我们确实能接触到源代码该怎么办呢？", "我访问了 ", "频道，并搜索了“验证码”。第一条搜索结果是 Really Simple CAPTCHA，并且有超过一百万次的活跃安装：", "最好的一点是，它是开源的！既然我们已经有了生成", "的源代码，那它应该挺容易被攻破的。为了让这件事更有挑战性，让我们给自己规定个时限吧。我们能在 15 分钟内完全攻破这个", "系统吗？来试试吧！", "为了构思一个攻击计划，来看看 ", " 会生成什么样的图片。在示例网站上，我们看到了以下图片：", "好了，所以", "似乎是四个字母。在 PHP 源代码中对其进行验证：", "没错，它用四种不同字体的随机组合来生成四个字母的", "。并且可以看到，它在代码中从未使用 O 或者 I，以此避免用户混淆。总共有 32 个可能的字母和数字需要我们识别。没问题！", "在进行下一步前，提一下我们要用来解决问题的工具：", "Python 是一种有趣的编程语言，它有大量的机器学习和计算机视觉库。", "OpenCV 是一种流行的计算机视觉和图片处理框架。我们要使用 OpenCV 来处理", "图片。由于它有 Python API，所以我们可以直接从 Python 中使用它。", "Keras 是用 Python 编写的深度学习框架。它使得定义、训练和用最少的代码使用深度神经网络容易实现。", "TensorFlow 是 Google 的机器学习库。我们会用 Keras 编程，但是 Keras 并没有真正实现神经网络的逻辑本身，而是在幕后使用 Google 的 TensorFlow 库来挑起重担。", "好了，回到我们的挑战吧！", "为了训练任何机器学习系统，我们需要训练数据。为了攻破一个", "系统，我们想要像这样的训练数据：", "鉴于我们有 WordPress 插件的源代码，我们可以调整它，一起保存 10,000 张", "图片及分别对应的答案。", "经过几分钟对代码的攻击，并添加了一个简单的 for 循环之后，我有了一个训练数据的文件夹——10,000 个 PNG 文件，文件名为对应的正确答案：", "这是唯一一个我不会给你示例代码的部分。我们做这个是为了教育，我不希望你们真去黑 WordPress 网站。但是，我最后会给你生成的这10,000 张图片，这样你就能重复我的结果了。", "既然有了训练数据，就可以直接用它来训练神经网络了：", "有了足够的训练数据，这个方法可能会有用——但是我们可以使问题更简化来解决。问题越简单，要解决它需要的训练数据就越少，需要的计算能力也越低。毕竟我们只有 15 分钟！", "幸运的是，", "图片总是由仅仅四个字母组成。如果我们能想办法把图片分开，使得每个字母都在单独的图片中，这样我们只需要训练神经网络一次识别一个字母：", "我没有时间去浏览 10,000 张训练图片并在 Photoshop 中手动把它们拆分开。这得花掉好几天的时间，而我只剩下 10 分钟了。我们还不能把图片分成相等大小的四块，因为该", "插件把字母随机摆放在不同的水平位置上以防止这一做法：", "幸运的是，我们仍然可以自动处理。在图像处理中，常常需要检测有相同颜色的像素块。这些连续像素块周围的界限被称为轮廓。OpenCV 中有一个 ", "() 函数，可以被用来检测这些连续区域。", "所以我们用一个未经处理的验证码图片开始：", "接下来把该图片转换成纯黑白（这叫做 thresholding），这样容易找到连续区域：", "接着，使用 OpenCV 的 ", "() 函数来检测该图片中包含相同颜色像素块的不同部分：", "接下来就是简单地把每个区域存成不同的图片文件。鉴于我们知道每张图片都应该包含从左到右的四个字母，我们可以利用这一点在保存的同时给字母标记。只要我们是按顺序保存的，我们就应该能保存好每个图片字母及其对应的字母名。", "但是等等——我看到一个问题！有时", "中有像这样重叠的字母：", "这意味着我们会把两个字母分离成一个区域：", "如果不处理这个问题，会创造出糟糕的训练数据。我们得解决这个问题，这样就不会意外地教机器把两个重叠的字母识别成一个字母了。", "一个简单的方法是，如果一个轮廓区域比它的高度更宽，这意味着很可能有两个字母重叠在一起了。在这种情况下，我们可以把重叠的字母从中间拆分成两个，并将其看作两个不同的字母：", "既然我们找到拆分出单个字母的方法了，就对所有", "图片进行该操作。目标是收集每个字母的不同变体。我们可以将每个字母保存在各自对应的文件夹中，以保持条理。", "在我分离出所有字母后，我的 W 文件夹长这样：", "由于我们只需要识别单个字母和数字的图片，我们不需要非常复杂的神经网络结构。识别字母要比识别像猫狗这样复杂的图片容易得多。", "我们要使用简单的卷积神经网络结构，有两层卷积层以及两层完全连接层：", "如果你想要了解更多神经网络的工作，以及为什么它们是图片识别的理想工具，请参考 ", "或者", "。", "定义该神经网络结构，只需要使用 Keras 的几行代码：", "现在我们可以训练它了！", "在 10 通过了训练数据集后，我们达到了几乎 100% 的正确率。此时，我们应该能随时自动绕过这个", "了！我们成功了！", "既然有了一个训练后的神经网络，利用它来攻破真实的验证码要很容易了：", "在破解验证码时，我们的模型看起来是这样：", "或者从命令来看：", "如果你想自己试试，你可以从这里", "（ http://t.cn/R8yFJiN ）。它包含 10,000 张示例图片和文章中每一步的所有代码。参考文件 README.md 中的运行指导。", "但是如果你想了解每一行代码都做了什么，我强烈建议你看看《 ", "。该书覆盖了更多的细节，而且有大量的详细示例。这本书是我目前见过的唯一一本既包含了运行原理，又包含了如何在现实生活中用其来解决复杂问题的书。去看看吧！"], "art_url": "http://python.jobbole.com/89004/", "art_img_url": "http://jbcdn2.b0.upaiyun.com/2018/01/86b4998ac58e5925e767a18e41b5438f.png", "art_title": "15 分钟用 ML 破解一个验证码系统"}
{"create_time": "2017-12-19", "art_content": ["唐纳德·克努特（Donald Knuth）曾经说过：“不成熟的优化方案是万恶之源。”然而，任何一个承受高负载的成熟项目都不可避免地需要进行优化。在本文中，我想谈谈优化Web项目代码的五种常用方法。虽然本文是以Django为例，但其他框架和语言的优化原则也是类似的。通过使用这些优化方法，文中例程的查询响应时间从原来的77秒减少到了3.7秒。", "本文用到的例程是从一个我曾经使用过的真实项目改编而来的，是性能优化技巧的典范。如果你想自己尝试着进行优化，可以在GitHub上获取优化前的初始代码，并跟着下文做相应的修改。我使用的是Python 2，因为一些第三方软件包还不支持Python 3。", "这个Web项目只是简单地跟踪每个地区的房产价格。因此，只有两种模型：", "抽象类", "提供了一个继承自模型并包含", "属性的模型，这个属性包含了实例的主键和模型的内容类型。 这能够隐藏像实例ID这样的敏感数据，而用散列进行代替。如果项目中有多个模型，而且需要在一个集中的地方对模型进行解码并要对不同类的不同模型实例进行处理时，这可能会非常有用。 请注意，对于本文的这个小项目，即使不用散列也照样可以处理，但使用散列有助于展示一些优化技巧。", "这是", "类：", "由于我们想通过API来提供这些数据，所以我们安装了Django REST框架并定义以下序列化器和视图：", "现在，我们将用一些数据来填充数据库（使用", "生成10万个房屋的实例：一个地区5万个，另一个4万个，第三个1万个），并准备测试应用程序的性能。", "在一个项目中我们需要测量下面这几个方面：", "但是，并不是所有这些都要用来度量项目的执行情况。一般来说，有两个指标比较重要：执行多长时间、需要多少内存。", "在Web项目中，", "（服务器接收由某个用户的操作产生的请求，处理该请求并返回结果所需的总的时间）通常是最重要的指标，因为过长的响应时间会让用户厌倦等待，并切换到浏览器中的另一个选项卡页面。", "在编程中，分析项目的性能被称为", "。为了分析API的性能，我们将使用", "包。在安装完这个包，并调用", "后，可以得到如下的结果：", "整体响应时间为77秒，其中16秒用于查询数据库，总共有5万次查询。这几个数字很大，提升空间也有很大，所以，我们开始吧。", "性能优化最常见的技巧之一是对数据库查询进行优化，本案例也不例外。同时，还可以对查询做多次优化来减小响应时间。", "仔细看一下这5万次查询查的是什么：都是对", "表的查询：", "时间戳 表名 联合 执行时间（毫秒）", "这个问题的根源是，Django中的查询是", "。这意味着在你真正需要获取数据之前它不会访问数据库。同时，它只获取你指定的数据，如果需要其他附加数据，则要另外发出请求。", "这正是本例程所遇到的情况。当通过", "来获得查询集时，Django将获取特定地区的所有房屋。但是，在序列化一个", "实例时，", "需要房子的", "实例来计算序列化器的", "字段。由于地区数据不在查询集中，所以django需要提出额外的请求来获取这些数据。对于查询集中的每一个房子都是如此，因此，总共是五万次。", "当然，解决方案非常简单。为了提取所有需要的序列化数据，你可以在查询集上使用", "。因此，", "函数将如下所示：", "我们来看看这对性能有何影响：", "总体响应时间降至36秒，在数据库中花费的时间约为100ms，只有4个查询！这是个好消息，但我们可以做得更多。", "默认情况下，Django会从数据库中提取所有字段。但是，当表有很多列很多行的时候，告诉Django提取哪些特定的字段就非常有意义了，这样就不会花时间去获取根本用不到的信息。在本案例中，我们只需要5个字段来进行序列化，虽然表中有17个字段。明确指定从数据库中提取哪些字段是很有意义的，可以进一步缩短响应时间。", "Django可以使用", "和", "这两个查询方法来实现这一点。第一个用于指定哪些字段", "，第二个用于指定", "哪些字段。", "这减少了一半的查询时间，非常不错。总体时间也略有下降，但还有更多提升空间。", "你不能无限制地优化数据库查询，并且上面的结果也证明了这一点。即使把查询时间减少到0，我们仍然会面对需要等待半分钟才能得到应答这个现实。现在是时候转移到另一个优化级别上来了，那就是：", "。", "有时，第三方软件包对于简单的任务来说有着太大的开销。本文例程中返回的序列化的房子实例正说明了这一点。", "Django REST框架非常棒，包含了很多有用的功能。但是，现在的主要目标是缩短响应时间，所以该框架是优化的候选对象，尤其是我们要使用的序列化对象这个功能非常的简单。", "为此，我们来编写一个自定义的序列化器。为了方便起见，我们将用一个静态方法来完成这项工作。", "现在看起来好多了，由于没有使用DRF序列化代码，所以响应时间几乎减少了一半。", "另外还有一个结果：在请求/响应周期内完成的总的函数调用次数从15,859,427次（上面1.2节的请求次数）减少到了9,257,469次。这意味着大约有三分之一的函数调用都是由Django REST Framework产生的。", "上述几个优化技巧是最常见的，无需深入地分析和思考就可以做到。然而，17秒的响应时间仍然感觉很长。要减少这个时间，需要更深入地了解代码，分析底层发生了什么。换句话说，需要分析一下代码。", "你可以自己使用Python内置的分析器来进行分析，也可以使用一些第三方软件包。由于我们已经使用了", "，它可以分析代码并生成一个二进制的分析文件，因此，我们可以做进一步的可视化分析。有好几个可视化软件包可以将二进制文件转换为一些友好的可视化视图。本文将使用", "。", "这是上文一个请求的二进制分析文件的可视化图表：", "从上到下是调用堆栈，显示了文件名、函数名及其行号，以及该方法花费的时间。可以很容易地看出，时间大部分都用在计算散列上（紫罗兰色的", "和", "矩形）。", "目前，这是代码的主要性能瓶颈，但同时，这不是我们自己写的代码，而是用的第三方包。", "在这种情况下，我们可以做的事情将非常有限：", "幸运的是，我们找到了一个更新版本的", "包。原代码使用的是v.2.1.0，而新的是v.3.0.4。", "当查看v.3的发行说明时，这一句话看起来令人充满希望：", "让我们来看一下！", "响应时间从17秒缩短到了8秒以内。太棒了！但还有一件事我们应该来看看。", "到目前为止，我们已经改进了查询、用自己特定的函数取代了第三方复杂而又泛型的代码、更新了第三方包，但是我们还是保留了原有的代码。但有时，对现有代码进行小规模的重构可能会带来意想不到的结果。但是，为此我们需要再次分析运行结果。", "仔细看一下，你可以看到散列仍然是一个问题（毫不奇怪，这是我们对数据做的唯一的事情），虽然我们确实朝这个方向改进了，但这个绿色的矩形表示", "花了2.14秒的时间，同时伴随着灰色的", "。这意味着初始化工作需要很长的时间。", "我们来看看", "包的源代码。", "正如你所看到的，一个", "实例的初始化需要调用", "函数，这是太重了，我们可以在上面的可视化图表中看到左下角的矩形。", "我们再来看看", "类：", "正如你所看到的，我已经标记了这两个方法初始化", "实例的方法，这并不是真正需要的。", "由于散列是一个确定性的过程，这意味着对于一个给定的输入值，它必须始终生成相同的散列值，因此，我们可以把它作为类的一个属性。让我们来看看它将如何执行：", "最后的结果是在4秒钟之内，比我们一开始的时间要小得多。对响应时间的进一步优化可以通过使用缓存来实现，但是我不会在这篇文章中介绍这个。", "性能优化是一个分析和发现的过程。 没有哪个硬性规定能适用于所有情况，因为每个项目都有自己的流程和瓶颈。 然而，你应该做的第一件事是分析代码。 如果在这样一个简短的例子中，我可以将响应时间从77秒缩短到3.7秒，那么对于一个庞大的项目来说，就会有更大的优化潜力。"], "art_url": "http://python.jobbole.com/88971/", "art_img_url": "http://jbcdn2.b0.upaiyun.com/2017/12/a53e7beb8e5e1b23fa86c861187e62ad.png", "art_title": "Python Django 性能测试与优化指南"}
{"create_time": "2018-01-25", "art_content": ["前段时间因为课题需要使用了一段时间TensorFlow，感觉这种框架很有意思，除了可以搭建复杂的神经网络，也可以优化其他自己需要的计算模型，所以一直想自己学习一下写一个类似的图计算框架。前几天组会开完决定着手实现一个模仿TensorFlow接口的简陋版本图计算框架以学习计算图程序的编写以及前向传播和反向传播的实现。目前实现了前向传播和反向传播以及梯度下降优化器，并写了个优化线性模型的例子。", "代码放在了GitHub上，取名", ", 仓库链接: ", "虽然前向传播反向传播这些原理了解起来并不是很复杂，但是真正着手写起来才发现,里面还是有很多细节需要学习和处理才能对实际的模型进行优化(例如Loss函数对每个计算节点矩阵求导的处理)。其中SimpleFlow的代码并没有考虑太多的东西比如", "和张量", "的检查等，因为只是为了实现主要图计算功能并没有考虑任何的优化, 内部张量运算使用的Numpy的接口(毕竟是学习和练手的目的嘛)。好久时间没更新博客了，在接下来的几篇里面我将把实现的过程的细节总结一下，希望可以给后面学习的童鞋做个参考。", "本文主要介绍计算图以及前向传播的实现, 主要涉及", "以及通过对构建好的图进行", "然后进行前向传播计算得到具体节点上的输出值。", "先贴上一个简单的实现效果吧:", "计算图是计算代数中的一个基础处理方法，我们可以通过一个有向图来表示一个给定的数学表达式，并可以根据图的特点快速方便对表达式中的变量进行求导。而神经网络的本质就是一个多层复合函数, 因此也可以通过一个图来表示其表达式。", "本部分主要总结计算图的实现，在计算图这个有向图中，每个节点代表着一种特定的运算例如求和，乘积，向量乘积，平方等等… 例如求和表达式$", "使用有向图表示为:", " ", "表达式$", "使用有向图表示为:", "与TensorFlow的实现不同，为了简化，在SimpleFlow中我并没有定义", "类来表示计算图中节点之间的数据流动，而是", "，其中主要定义了四种类型来表示图中的节点:", "其实图中的所有节点都可以看成是某种操作，其中", ", ", ", ", "都是一种特殊的操作，只是相对于普通的", "而言，他们没有输入，但是都会有输出（像上图中的", ", ", "节点，他们本身输出自身的值到", "节点中去），通常会输出到", "节点，进行进一步的计算。", "下面我们主要介绍如何实现计算图的基本组件: 节点和边。", "节点表示操作，边代表节点接收和输出的数据，操作节点需要含有以下属性:", "下面我们定义了", "基类用于表示图中的操作节点(详见", "):", "在初始化方法中除了定义上面提到的属性外，还需要进行两个操作:", "另外，每个操作节点还有两个必须的方法: ", "和", ". 他们分别负责根据输入节点的值计算当前节点的输出值和根据操作属性和当前节点的值计算梯度。关于梯度的计算将在后续的文章中详细介绍，本文只对节点输出值的计算进行介绍。", "下面我以", "操作为例来说明具体操作节点的实现:", "可见，计算当前节点", "的值的", "就是", "。", "与", "节点类似，", "节点也需要", ", ", "等属性，但是它没有输入节点，也就没有", "属性了，而是需要在创建的时候确定一个初始值", ":", "和", "节点与", "节点类似，具体实现详见: ", "在定义了图中的节点后我们需要将定义好的节点放入到一个图中统一保管，因此就需要定义一个", "类来存放创建的节点，方便统一操作图中节点的资源。", "为了提供一个默认的图，在导入simpleflow模块的时候创建一个全局变量来引用默认的图:", "为了模仿TensorFlow的接口，我们给", "添加上下文管理器协议方法使其成为一个上下文管理器, 同时也添加一个", "方法:", "这样在进入", "代码块之前先保存旧的默认图对象然后将当前图赋值给全局图对象，这样", "代码块中的节点默认会添加到当前的图中。最后退出", "代码块时再对图进行恢复即可。这样我们可以按照TensorFlow的方式来在某个图中创建节点.", "Ok，根据上面的实现我们已经可以创建一个计算图了:", "实现了计算图和图中的节点，我们需要对计算图进行计算, 本部分对计算图的前向传播的实现进行总结。", "首先，我们需要实现一个", "来对一个已经创建好的计算图进行计算，因为当我们创建我们之前定义的节点的时候其实只是创建了一个空节点，节点中并没有数值可以用来计算，也就是", "是空的。为了模仿TensorFlow的接口，我们在这里也把session定义成一个上下文管理器:", "上面我们已经可以构建出一个计算图了，计算图中的每个节点与其相邻的节点有方向的联系起来，现在我们需要根据图中节点的关系来推算出某个节点的值。那么如何计算呢? 还是以我们刚才￥", "的计算图为例,", "若我们需要计算橙色", "运算节点的输出值，我们需要计算与它相连的两个输入节点的输出值，进而需要计算绿色", "的输入节点的输出值。我们可以通过后序遍历来获取计算一个节点所需的所有节点的输出值。为了方便实现，后序遍历我直接使用了递归的方式来实现:", "通过此函数我们可以获取计算一个节点值所需要所有节点列表，再依次计算列表中节点的输出值，最后便可以轻易的计算出当前节点的输出值了。", "上面我们实现了计算图以及前向传播，我们就可以创建计算图计算表达式的值了, 如下:", "输出值:", "本文使用Python实现了计算图以及计算图的前向传播，并模仿TensorFlow的接口创建了", "以及", "对象。下篇中将继续总结计算图节点计算梯度的方法以及反向传播和梯度下降优化器的实现。", "最后再附上simpleflow项目的链接, 欢迎相互学习和交流: "], "art_url": "http://python.jobbole.com/88998/", "art_img_url": "http://jbcdn2.b0.upaiyun.com/2018/01/f17481e084e126080cf07fdb1a43d451.jpg", "art_title": "实现属于自己的TensorFlow(1)：计算图与前向传播"}
{"create_time": "2017-12-27", "art_content": ["数据的重要性毋庸置疑，但是如何让数据产生价值呢？", "对一个全栈老码农而言，经常在开发或者研发管理的时候遇到各种预测、决策、推断、分类、检测、排序等诸多问题。面对“你的代码还有 bug 么？”这样的挑战，一种理智的回答是，我们已经执行了若干测试用例，代码中存在bug的可能性是百分之零点几。也就是说，我们对当前程序中没有bug的信心是百分之九十九点几。这实际上就是一直贝叶斯思维，或者说使用了贝叶斯方法。不论我们看到，还是没有看到，它都在那里，熠熠生辉。", "如果预测当前软件有没有bug呢？还是要从贝叶斯定理看起。", "对老码农来说，贝叶斯定理的概率表达相对清晰，理解起来会相对容易。回忆一下我们学过的概率论，联合概率是满足交换律的，即：", "对联合概率以条件概率展开：", "从而得到：", "简单的变换一下，得到：", "大功告成，这就是神奇的贝叶斯定理。其中：", "还可以加点料，在计算P（A）的时候，可以用加法定理表示：", "从而有：", "其中B_ 是与B相反的事件。就测试与bug 之间的估算而言，《", "》一文给出了贝叶斯推断的结果，其中就使用了这样的方法。", "贝叶斯方法是一个非常通用的推理框架，用客观的新信息更新我们最初关于某个事物的信念后，就会得到一个新的改进了的信念。通过引入先验的不确定性，允许了初始推断的错误，获得了更新的证据后，也没有放弃初始的推断，而是调整为更符合目前的证据。", "但是，P（A|B） 和 P（B|A） 之类的经常让人混淆，@待字闺中的陈老师给出了理解的一个关键点，区分出规律和现象，就是将A看成“规律”，B看成“现象”，那么贝叶斯公式看成：", " ", "陈老师在《这的理解贝叶斯公式吗》和《又一个生活中的贝叶斯应用》给出了几个通俗易懂的例子，这里不再赘述。", "回归到码农生活，我们在改善系统功能的时候，通常的一个手段是AB测试。AB测试是用来检测两种不同处理方式的差异化程度的一种统计设计模式，例如两个网站谁会带来更高的转化率，这里的转化可以是用户的购买、注册、或其他的行为。AB测试的关键点在于组别之间只能容许一个不同点。实验后的分析一般都是用假设检验完成的，例如均值差异检验或者比例差异检验，往往涉及Z分数或令人困惑的p值，而用贝叶斯方法则会自然的多。", "对A，B两个网站的转化概率进行建模。转化率在0～1之间，可采用Beta分布。如果先验是Beta（a1，b1），且 观测到N次访问里有X次转化，那么此时的后验分布是Beta（a1+X,b1+N-X). 假设先验是Beta（1，1），等价于【0，1】上的均匀分布，则示例代码如下：", "使用贝叶斯方法，是从思考数据是如何产生的开始。", "\n1）什么随机变量能过描述这些统计数据", "\n2）确实概率分布的所需参数", "\n3）参数对应早期行为，或后期行为，定义各种变化点", "\n4）定义参数的概率分布", "\n5）参数概率分布的变量选择，直到一个可以假设的均匀分布", "对先验及后验概率的选择，针对应用场景而定。就先验分布而言，除了常见的分布外，还有：", "\n* Gamma分布，指数随机变量的推广", "\n* 威沙特分布 ，是所有半正定矩阵的分布，是一个协方差矩阵的适当的先验。", "\n* Beta分布，随机变量定义在0到1之间，使其成为概率和比例的热门选择。", "\n* 幂律分布，满足公司规模和公司数量之间的关系", "在AB测试中使用了Beta分布， 应用了一个Beta先验分布连同二项式生成的观测数据形成一个Beta后验分布这一原理。", "当面对多种对象之间的因果关系的时候，贝叶斯方法演变成为了贝叶斯网络。", "贝叶斯网络是为了解决不定性和不完整性问题而提出的，在多个领域中获得了广泛应用。贝叶斯网络是基于概率推理的图形化网络，而贝叶斯公式则是这个概率网络的基础。贝叶斯网络中的每个点代表一个随机变量，都是具有实际含义、需要人为设计的，点和点之间的边代表不确定的因果关系，例如 节点E直接影响到节点H，即E→H，则用从E指向H的箭头建立结点E到结点H的有向弧(E,H)，权值(即连接强度)用条件概率P(H|E)来表示。", "实际上，如果事物之间的关系能够用一条链串起来，形成了贝叶斯网络的一个特例——马尔可夫链，换个角度看， 贝叶斯网络是马尔可夫链的非线性扩展。贝叶斯网络中当某点的一个证据出现后，整个网络中事件的概率都会变化。", "简单地，由于多个变量间存在着可能的依赖性，贝叶斯网络说明了其中的联合条件概率分布，允许在变量的子集间定义条件独立性。使用贝叶斯网络的过程与使用贝叶斯方法的过程是类似的：", "例如， 社交网络中不真实账户的检测问题。首先确定网络中的随机变量：", "\n* 账户的真实性 A", "\n* 头像的真实性 H", "\n* 发帖即日志的密度 L", "\n* 好友的密度 F", "使用观测值示例化H，L，F，把随机值赋给A，得到", "P（A|H,L,F) = P(H|A)P(L|A)P(F|A,H)", "然后就可以在社交网络中尝试使用该推理结果了。在《算法杂货铺——分类算法之贝叶斯网络》一文中对这一例子给出了相对详细的说明。", "可以说，贝叶斯方法席卷了整个概率论，并将应用延伸到各个问题领域，所有需要作出概率预测的地方都可以见到贝叶斯方法的影子，特别地，贝叶斯方法对机器学习能够有什么帮助呢？", "机器学习在业界炙手可热，但我们在机器学习里同样会遇到预测、决策、分类、检测等问题，贝叶斯方法同样大有用武之地。", "机器学习中有大量的模型，如线性模型、非线性模型，可以采用贝叶斯方法来做模型的预测。也就是说，某一场景可能采用的模型是无限多的，可以用概率分布去描述它。对于假设的先验，对新来的样本做预测如计算它的似然，然后用前面推出来的后验分布做积分，这个给定模型下样本的似然，就是所有可能模型的分布。", "机器学习中模型的选择和比较也是一个常见的问题。例如，在分类问题时，我们使用线性模型还是深度学习的非线性模型呢？贝叶斯方法是这样考虑的： 用A 表示一个模型类别，可能是线性模型，B 表示另一个模型类别，可能是非线性模型。在同样的数据集X下，计算在A，B 情况下观察到训练集的似然Ma，Mb，然后比较Ma和Mb，这是贝叶斯方法做模型选择的一个基本规则。", "实际上， 贝叶斯定理是信息处理的一种准则， 输入是一个先验分布和一个似然函数，输出是一个后验分布。对机器学习中的模型本身，也可以通过贝叶斯方法尝试改进，例如贝叶斯SVM, 高斯过程的贝叶斯等等。", "另外，贝叶斯方法对深度学习而言，至少在调参的这一环节还是很有用的。在神经网络中，每一层参数如卷积核的大小和数量等，都不会在深度学习中被模型自动优化的，需要手工指定，这或许就是贝叶斯优化。", "感慨一下，码农不识贝叶斯，虽知数据也枉然呀！"], "art_url": "http://python.jobbole.com/88318/", "art_img_url": "http://jbcdn2.b0.upaiyun.com/2015/02/301f8986e2ad36a068277f2795edeeb9.jpg", "art_title": "码农不识贝叶斯，虽知数据也枉然"}
{"create_time": "2017-12-24", "art_content": ["你好！ 我作为一名编写Ruby profiler的先驱，我想对现有的Ruby和Python profiler如何工作进行一次调查。 这也有助于回答很多人的问题：“你怎么写一个profiler？”", "在这篇文章中，我们只关注CPUprofiler（而不是内存/堆profiler）。 我将解释一些编写profiler的一般基本方法，给出一些代码示例，以及大量流行的Ruby和Pythonprofiler的例子，并告诉你它们是如何工作的。", "在这篇文章中可能会有一些错误（为了研究这篇文章，我阅读了14个不同的分析库的代码部分），请让我们开始吧！", " ", "有两种基本CPU profilers类型 – ", "profilers和", "profilers。", "tracingprofilers记录您的程序所调用的每个函数，然后在最后打印出报告。 samplingprofilers采用更加统计化的方法 – 他们每隔几毫秒记录程序的堆栈情况，然后报告结果。", "使用sampling profilers而不是tracing profilers的主要原因是sampling profilers的开销较低。 如果每秒只抽取20或200个样本，那不会花费多少时间。 而且它们非常有效率 – 如果您遇到严重的性能问题（比如80％的时间花费在1个慢速函数上），那么每秒200个样本通常就足以确定那个函数的问题所在了！", "下边类出了我们这篇文章要讨论的分析器(", ")。我之后将会解释表格中的术语(setitimer, rb_add_event_hook, ptrace)。这里最有趣的是，所有的分析器都是通过一小部分函数的特性实现的。", "“gbd hacks”并不完全是一个Python分析器：它是一个讲述如何实现用脚本包装gdb来实现hacky分析器的链接。由于新版本的gdb事实上会展开Python堆栈，所以也是和Python有关的。一种简化版的pyflame。", "在我们开始详细分析这些分析器之前，有一个非常重要的事情需要说明一下：除fyflame外所有的分析器都运行在你的Python/Ruby进程里面。如果你在一个Python/Ruby程序里面，你通常可以很容易的获取该程序的堆栈。例如下边代码中的简单的Python程序答应出每一个运行线程的堆栈：", "你可以从下边的输出里面看到堆栈的函数名，行号，文件名等你在做分析的时候需要的所有信息。", "在Ruby程序中，获取堆栈也很容易：你只需要通过caoller来获取堆栈。", "这些分析器处于性能考虑都是C扩展所有它们有一点不一样，但是Ruby/Python程序的C扩展也可以很容易的获取调用堆栈。", "我调查过上边表格中所有的追踪分析器:rblineprof、ruby-prof和cProfile。它们工作原理基本相同。它们都记录所有的函数调用并且用C语言编写来降低耗时。", "它们是如何工作的呢？Ruby和Python都允许指定一个回调函数，当各种解释事件(例如调用一个函数或者执行一行代码)发生的时候调用。当回调函数被调用的时候，会记录堆栈供以后分析。", "我认为确切了解在代码中哪里设置这些回调函数是很有用的，所以我连接了所有在github上边的相关代码。", "在Python中，可以通过PyEval_SetTrace或者 PyEval_SetProfile设置回调函数。在Python官方文档的", "里有说明。文档中说道：除了追踪函数会收到line-number事件外“PyEval_SetTrace和PyEval_SetProfile一样。", "在Ruby里，你可以用rb_add_event_hook来设置回调，我找不到任何关于此处是如何调用的文档", "prof_event_hook的类型是", "这看起来像极了Python的PyEval_SetTrace，但是比Python更灵活——您可以选择你关注的事件类型（就像“函数调用”一样）。", "追踪分析器的主要的缺点是它的实现方式是对于每个函数/行代码都执行固定的次数，这样可能使你做出错误的决定。例如，如果你有某个事物的两个实现：一个通过大量的函数调用实现，另一个没有大量函数调用，两个实现耗时相同，有大量函数调用的相比没有大量函数调用的在分析的时候会变得慢。", "为了测试这一点，我做了一个包含下边内容的小文件test.py，并且比较了python -mcProfile test.py和python test.py的耗时。python test.py执行需要大约0.6秒，python -mcProfile test.py执行需要大约1秒。对于这个特定的例子cProfile引入了额外的大约60%的开销。", "cProfile文档中说：", "这似乎是一个合理的说法：上边的示例(执行350万次函数调用)显然不是个典型的Python程序，并且几乎任何其他程序开销都比该示例小。", "我没有测试ruby-prof(一个ruby追踪分析器)的开销，但是它的README说：", "现在讨论第二种分析器：采样分析器。", "大多数Ruby和Python的采样分析器都是通过系统调用setitimer实现的。这是怎么回事呢？", "好吧，比方说你想要每秒获取一个程序的堆栈50次，一种方法是：", "如果你想要看一个实际的用setitimer实现采样分析器的例子的话，我认为", "是一个最好的例子，stacksampler.py是一个有用的有效的分析器并且代码只有大约100行，好酷啊！", "stacksampler.py只有100多行的一个原因是：当你把一个Python函数注册成信号处理器的时候，该函数被传送到你的Python程序的当前堆栈中。所以stacksampler.py信号处理器注册是非常简单的：", "它只是将堆栈从堆栈帧中取出来并且增加堆栈查看计数，非常简单！非常酷！", "我们看继续剩下的使用setitimer的分析器并找到它们调用settimer的代码：", "关于setitimer很重要的一点是，你需要决定", "。你想要真正的20 ms的“挂钟”时间？你想要20 ms的用户CPU时间？或者20 ms的用户+系统CPU时间？如果你仔细看电话网站上的内容，你就会发现，这些分析器实际上对setitimer做出了不同的选择 — 有时候它是可配置的，有时候却不可。setitimer手册页十分精悍，并且值得去读懂上面所有的观点。", " 在推特上指出了一个使用setitimer时出现的有趣的问题，这个问题和这个问题拥有的一系列更多细节。", "有些采样分析器不使用setitimer：", "所有这3个分析器使用挂钟定时采样。", " ", "有很多关于pyflame是如何工作的。我不打算在这里进行介绍，但是Evan Klitke写了很多关于它的非常好的博客：", "还有很多在 ", "。所有有趣的东西，我会更详细地阅读——也许ptrace是比实现一个Ruby分析器process_vm_readv更好的方法！（process_vm_readv开销低，因为它不会阻断进程，但它也可以给你一个不一致的快照，因为它不会阻断进程：））", " ", "在这篇文章中我没有涉及很多重要的细节 – 比如我基本上说vmprof和stacksampler是一样的（但实际上它们不是 – vmprof支持线性分析和用C语言编写的Python函数分析，我相信这在分析器中引入了更多的复杂性）。 但一些基本原理是一样的，所以我认为这项调查是一个很好的起点。"], "art_url": "http://python.jobbole.com/88977/", "art_img_url": "http://jbcdn2.b0.upaiyun.com/2015/02/591d8b55a524f825dd29a22b8df70000.jpg", "art_title": "Ruby 和 Python 分析器是如何工作的？"}
{"create_time": "2017-11-28", "art_content": ["第一个问题，什么是 Python ？根据 Python 之父 Guido van Rossum 的话，Python 是：", "对于我来说，学习 Python 的首要原因是，Python 是一种可以优雅编程的语言。它能够简单自然地写出代码和实现我的想法。", "另一个原因是我们可以将 Python 用在很多地方：数据科学、Web 开发和机器学习等都可以使用 Python 来开发。Quora、Pinterest 和 Spotify 都使用 Python 来进行他们的后端 Web 开发。那么让我们来学习一下 Python 吧。", "你可以把变量想象成一个用来存储值的单词。我们看个例子。", "Python 中定义一个变量并为它赋值是很容易的。假如你想存储数字 1 到变量 “one” ，让我们试试看：", "超级简单吧？你只需要把值 1 分配给变量 “one” 。", "只要你想，你可以把任意的", "赋给任何其他的", "。正如你从上面看到的那样，变量 “", "” 存储整型变量 ", "变量 “", "” 存储 10000 。", "除了整型，我们还可以使用布尔值（True/Flase）、字符串、浮点型和其他数据类型。", "“", "” 使用一个表达式来判断一个语句是 True 还是 False ，如果是 True ，那么执行 if 内的代码，例子如下：", "比 ", "大，所以 ", "代码被执行。", "当“", "”里面的表达式是 ", "时，“", "” 语句将会执行。", "比 ", "小，所以 “", "” 里面的代码会执行。", "你也可以使用 “", "” 语句：", "在 Python 中，我们可以用不同的形式进行迭代。我会说下 ", "和 ", "循环：当语句是 True 时，while 内部的代码块会执行。所以下面这段代码会打印出 ", "到 ", "循环需要", "，如果条件一直是 True ，它将会一直迭代，当 num 的值为 11 时，循环条件为 false 。", "另一段代码可以帮你更好的理解 while 语句的用法：", "循环条件是 True 所以会一直迭代，直到为 False 。", "：你可以在代码块上应用变量 “", "” ，而 “for” 语句将为你迭代它。此代码将打印与 ", "中相同的代码：从 1 到 10 。", "瞧见没？这太简单了。i 的范围从 1 开始一直到第 11 个元素（10是第十个元素）", "假如你想要在一个变量里存储整数 1 ，但是你也要存储 2 和 3 , 4 , 5 …", "不是用成百上千个变量，我有别的方法存储这些我想要存储的整数吗？你已经猜到了，确实有别的存储它们的方法。", "列表是一个集合，它能够存储一列值（就像你想要存储的这些），那么让我们来用一下它：", "这真的很简单。我们创建了一个叫做 my_integer 的数组并且把数据存到了里面。", "也许你会问：“我要怎样获取数组里的值？”", "问的好。列表有一个叫做索引的概念。第一个元素的下表是索引0（0）。第二个的索引是1，以此类推，你应该明白的。", "为了使它更加简洁，我们可以用它的索引代表数组元素。我画了出来：", "用 Python 的语法，也很好去理解：", "假如你不想存整数。你只想去存一些字符串，像你亲戚名字的列表。我的看起来是类似这样的：", "它的原理跟存整数一样，很友好。", "我们只学习了列表的索引是如何工作的，我还需要告诉你如何向列表的数据结构中添加一个元素（向列表中添加一个项目）。", "最常用的向列表中添加新数据的方法是拼接。我们来看一下它是如何使用的："], "art_url": "http://python.jobbole.com/88940/", "art_img_url": "http://jbcdn2.b0.upaiyun.com/2017/11/8ef4df4888b257b5ea7bbd4b033a519c.png", "art_title": "从 Zero 到 Hero ，一文掌握 Python"}
{"create_time": "2017-11-27", "art_content": ["本文除非特殊指明，”python“都是代表CPython，即C语言实现的标准python，且本文所讨论的是版本为2.7的CPython。另外，本文会不定期更新，如果大家有一些好的想法，请在评论里面留言，我会补充到文章中去。", "当我们提到一门编程语言的效率时：通常有两层意思，", "，这是对程序员而言，完成编码所需要的时间；", "，这是对计算机而言，完成计算任务所需要的时间。编码效率和运行效率往往是鱼与熊掌的关系，是很难同时兼顾的。不同的语言会有不同的侧重，python语言毫无疑问更在乎编码效率，life is short，we use python。", "虽然使用python的编程人员都应该接受其运行效率低的事实，但python在越多越来的领域都有广泛应用，比如科学计算 、web服务器等。程序员当然也希望python能够运算得更快，希望python可以更强大。", "首先，python相比其他语言具体有多慢，这个不同场景和测试用例，结果肯定是不一样的。", "给出了不同语言在各种case下的性能对比，", "是python3和C++的对比，下面是两个case：", "从上图可以看出，不同的case，python比C++慢了几倍到几十倍。", "python运算效率低，具体是什么原因呢，下列罗列一些", "一个变量所指向对象的类型在运行时才确定，编译器做不了任何预测，也就无从优化。举一个简单的例子：　", "　a和b相加，但a和b的类型在运行时才知道，对于加法操作，不同的类型有不同的处理，所以每次运行的时候都会去判断a和b的类型，然后执行对应的操作。而在静态语言如C++中，编译的时候就确定了运行时的代码。", "另外一个例子是属性查找，关于具体的查找顺序在", "中有详细介绍。简而言之，访问对象的某个属性是一个非常复杂的过程，而且通过同一个变量访问到的python对象还都可能不一样（参见Lazy property的例子）。而在C语言中，访问属性用对象的地址加上属性的偏移就可以了。", "，但是不支持JIT（just in time compiler）。虽然大名鼎鼎的google曾经尝试", " 这个项目，但最终也折了。", "，每个对象都需要维护引用计数，增加了额外的工作。", "GIL是Python最为诟病的一点，因为GIL，python中的多线程并不能真正的并发。如果是在IO bound的业务场景，这个问题并不大，但是在CPU BOUND的场景，这就很致命了。所以笔者在工作中使用python多线程的情况并不多，一般都是使用多进程（pre fork），或者在加上协程。即使在单线程，GIL也会带来很大的性能影响，因为python每执行", "（默认，可以通过sys.setcheckinterval()设置）就会尝试线程的切换，具体的源代码在ceval.c::PyEval_EvalFrameEx。", "，这个可能是所有具有垃圾回收的编程语言的通病。python采用标记和分代的垃圾回收策略，每次垃圾回收的时候都会中断正在执行的程序，造成所谓的顿卡。", "上有一篇文章，提到禁用Python的GC机制后，Instagram性能提升了10%。感兴趣的读者可以去细读。", "Be pythonic", "我们都知道 过早的优化是罪恶之源，一切优化都需要基于profile。但是，作为一个python开发者应该要pythonic，而且pythonic的代码往往比non－pythonic的代码效率高一些，比如：", "dict的iteritems 而不是items（同itervalues，iterkeys）", "使用generator，特别是在循环中可能提前break的情况", "即使我们的代码已经非常pythonic了，但可能运行效率还是不能满足预期。我们也知道", "了，优化的关键在于找出这些瓶颈代码。方式很多：到处加log打印时间戳、或者将怀疑的函数使用timeit进行单独测试，但最有效的是使用profile工具。", "对于python程序，比较出名的profile工具有三个：", "。其中profile是纯python语言实现的，Cprofile将profile的部分实现native化，hotshot也是C语言实现，hotshot与Cprofile的区别在于：hotshot对目标代码的运行影响较小，代价是更多的后处理时间，而且hotshot已经停止维护了。", "，profile（Cprofile hotshot）只适合单线程的python程序。", "对于多线程，可以使用", "，yappi不仅支持多线程，还可以精确到CPU时间", "对于协程（greenlet），可以使用", "，基于yappi修改，用greenlet context hook住thread context", "下面给出一段编造的”效率低下“的代码，并使用Cprofile来说明profile的具体方法以及我们可能遇到的性能瓶颈。", "运行结果如下：", "对于上面的的输出，每一个字段意义如下：", "代码中的输出非常简单，事实上可以利用pstat，让profile结果的输出多样化，具体可以参见官方文档", "。", "虽然Cprofile的输出已经比较直观，但我们还是倾向于保存profile的结果，然后用图形化的工具来从不同的维度来分析，或者比较优化前后的代码。查看profile结果的工具也比较多，比如，", "，本文用visualpytune做分析。对于上面的代码，按照注释生成修改后重新运行生成test.prof文件，用visualpytune直接打开就可以了，如下：", "\n字段的意义与文本输出基本一致，不过便捷性可以点击字段名排序。左下方列出了当前函数的calller（调用者），右下方是当前函数内部与子函数的时间占用情况。上如是按照cumtime（即该函数内部及其子函数所占的时间和）排序的结果。", "造成性能瓶颈的原因通常是", "。在我们前面的例子中，foo就属于高频调用的情况，bar属于单次消耗非常高的情况，这都是我们需要优化的重点。", "中介绍了qcachegrind和runsnakerun的使用方法，这两个colorful的工具比visualpytune强大得多。具体的使用方法请参考原文，下图给出test.prof用qcachegrind打开的结果", "qcachegrind确实要比visualpytune强大。从上图可以看到，大致分为三部：。第一部分同visualpytune类似，是每个函数占用的时间，其中Incl等同于cumtime， Self等同于tottime。第二部分和第三部分都有很多标签，不同的标签标示从不同的角度来看结果，如图上所以，第三部分的“call graph”展示了该函数的call tree并包含每个子函数的时间百分比，一目了然。", "知道了热点，就可以进行针对性的优化，而这个优化往往根具体的业务密切相关，没用万能钥匙，具体问题，具体分析。个人经验而言，最有效的优化是找产品经理讨论需求，可能换一种方式也能满足需求，少者稍微折衷一下产品经理也能接受。次之是修改代码的实现，比如之前使用了一个比较通俗易懂但效率较低的算法，如果这个算法成为了性能瓶颈，那就考虑换一种效率更高但是可能难理解的算法、或者使用", "模式。对于这些同样的方法，需要结合具体的案例，本文不做赘述。", "接下来结合python语言特性，介绍一些让python代码不那么pythonic，但可以提升性能的一些做法", "每一层函数调用都会带来不小的开销，特别对于调用频率高，但单次消耗较小的calltree，多层的函数调用开销就很大，这个时候可以考虑将其展开。", "对于之前调到的profile的代码，foo这个call tree非常简单，但频率高。修改代码，增加一个plain_foo()函数, 直接返回最终结果，关键输出如下：", "跟之前的结果对比：", "可以看到，优化了差不多3倍。", "上面提到，python 的属性查找效率很低，如果在一段代码中频繁访问一个属性（比如for循环），那么可以考虑用局部变量代替对象的属性。", "在本文的第一章节已经提到，关闭GC可以提升python的性能，GC带来的顿卡在实时性要求比较高的应用场景也是难以接受的。但关闭GC并不是一件容易的事情。我们知道python的引用计数只能应付没有循环引用的情况，有了循环引用就需要靠GC来处理。在python语言中, 写出循环引用非常容易。比如：", "当然，大家可能说，谁会这么傻，写出这样的代码，是的，上面的代码太明显，当中间多几个层级之后，就会出现“间接”的循环应用。在python的标准库 collections里面的OrderedDict就是case2：", "要解决循环引用，第一个办法是使用弱引用（weakref），第二个是手动解循环引用。", "如果程序确定是单线程，那么修改checkinterval为一个更大的值，", "有介绍。", "slots最主要的目的是用来节省内存，但是也能一定程度上提高性能。我们知道定义了__slots__的类，对某一个实例都会预留足够的空间，也就不会再自动创建__dict__。当然，使用__slots__也有许多注意事项，最重要的一点，继承链上的所有类都必须定义__slots__，python doc有详细的描述。下面看一个简单的测试例子：", "输出结果：", "也许通过profile，我们已经找到了性能热点，但这个热点就是要运行大量的计算，而且没法cache，没法省略。。。这个时候就该python的C扩展出马了，", "。由于C语言的效率远远高于python代码，所以使用C扩展是非常普遍的做法，比如我们前面提到的cProfile就是基于_lsprof.so的一层封装。python的大所属对性能有要求的库都使用或者提供了C扩展，如gevent、protobuff、bson。", "笔者曾经测试过纯python版本的bson和cbson的效率，在综合的情况下，cbson快了差不多10倍！", "python的C扩展也是一个非常复杂的问题，本文仅给出一些注意事项：", "这是最难最复杂的一点。我们都知道python基于指针技术来管理对象的生命周期，如果在扩展中引用计数出了问题，那么要么是程序崩溃，要么是内存泄漏。更要命的是，引用计数导致的问题很难debug。。。", "C扩展中关于引用计数最关键的三个词是：steal reference，borrowed reference，new reference。建议编写扩展代码之前细读python的", "。", "这里的多线程是指在扩展中new出来的C语言线程，而不是python的多线程，出了python doc里面的介绍，也可以看看《python cookbook》的相关章节。", "仅适合与业务代码的关系不那么紧密的逻辑，如果一段代码大量业务相关的对象 属性的话，是很难C扩展的", "将C扩展封装成python代码可调用的接口的过程称之为binding，Cpython本身就提供了一套原生的API，虽然使用最为广泛，但该规范比较复杂。很多第三方库做了不同程度的封装，以便开发者使用，比如", "（同时支持pypy cpython），具体怎么使用可以google。", "尽管python的性能差强人意，但是其易学易用的特性还是赢得越来越多的使用者，业界大牛也从来没有放弃对python的优化。这里的优化是对python语言设计上、或者实现上的一些反思或者增强。这些优化项目一些已经夭折，一些还在进一步改善中，在这个章节介绍目前还不错的一些项目。", "前面提到", "可以用到binding c扩展，但是其作用远远不止这一点。", "Cython的主要目的是加速python的运行效率，但是又不像上一章节提到的C扩展那么复杂。在Cython中，写C扩展和写python代码的复杂度差不多（多亏了", "）。Cython是python语言的超集，增加了对C语言函数调用和类型声明的支持。从这个角度来看，cython将动态的python代码转换成静态编译的C代码，这也是cython高效的原因。使用cython同C扩展一样，需要编译成动态链接库，在linux环境下既可以用命令行，也可以用distutils。", "如果想要系统学习cython，建议从", "入手，文档写得很好。下面通过一个简单的示例来展示cython的使用方法和性能（linux环境）。", "首先，安装cython：", "下面是测试用的python代码，可以看到这两个case都是运算复杂度比较高的例子：", "运行结果：", "不改动任何python代码也可以享受到cython带来的性能提升，具体做法如下：", "可以看到 增加了两个文件，对应中间结果和最后的动态链接库", "运行结果：", "性能提升了大概两倍，我们再来试试cython提供的静态类型（static typing），修改cython_example.pyx的核心代码，替换f()和integrate_f()的实现如下：", "然后重新运行上面的第三 四步：结果如下", "上面的代码，只是对参数引入了静态类型判断，下面对返回值也引入静态类型判断。", "替换f()和integrate_f()的实现如下：", "然后重新运行上面的第三 四步：结果如下", "Amazing！", "pypy是CPython的一个替代实现，其最主要的优势就是pypy的速度，下面是官网的测试结果：", "在实际项目中测试，pypy大概比cpython要快3到5倍！pypy的性能提升来自JIT Compiler。在前文提到google的", " 项目也是想在CPython中引入JIT，在这个项目失败后，很多开发人员都开始加入pypy的开发和优化。另外pypy占用的内存更少，而且支持stackless，基本等同于协程。", "pypy的缺点在于对C扩展方面支持的不太好，需要使用CFFi来做binding。对于使用广泛的library来说，一般都会支持pypy，但是小众的、或者自行开发的C扩展就需要重新封装了。", "2017.03.10 增加了对__slots__的介绍"], "art_url": "http://python.jobbole.com/88926/", "art_img_url": "http://jbcdn2.b0.upaiyun.com/2017/11/7acbaf502a752190a99c5c9f68548d53.png", "art_title": "Python 性能优化"}
{"create_time": "2017-11-23", "art_content": ["11 月 6 日，湖南卫视已经开播被称作年度压轴的大戏“猎场”，迅速占领各大榜单，成为一部高热度的电视剧。但是在豆瓣上却形成了两极分化。截止 11 月 8 日，该剧在豆瓣上的评分为 5.7 分。相比较胡歌之前《琅琊榜》的 9.1，《伪装者》的 8.3 等来说，这一评分确实不高。有趣的是，首页的评分比例与“短评”、“剧评”的比例存在非常大的差异！", "首页总评分评分两级分化严重，“差评”占主 在目前 11463 个评价中两级分化严重，“1 星”占比最高为 28.6%，其次为“5 星”的 25.4%。“好评”（5 星、4 星）占比为 35.80%，“一般”（3 星）为 16.50%，“差评”（2 星、1 星）占比为 47.80%。很明显，“差评”占了接近一半的比例。", "《猎场》豆瓣评分占比分布", "在短评和剧评中的另一种景象 首页的豆瓣评分中“差评”占比很高，但是在豆瓣的短评和剧评中却是另一番景象。 在目前 5979 条短评中，“好评”占比 71%，“一般”为 5%，“差评”占比 24%。而在 392 条剧评中，“5 星”占了非常高的比例！84.7%的剧评给了“好评”。", "《猎场》剧评评分分布", "我们将三个位置的评分放在一起比较就会出现非常明显的差异。根据这个差异，我们可以大致判断：写出短评或者剧评的观众大部分给予了“好评”，但仍有大量观众直接给了差评，并没有说明任何原因。当然，我们并没有考虑那些不写评论，而只是点“有用”和“没用”观众。", "才刚刚上映，剧情还在慢慢的铺，所以现在给整部剧下定论还太早。", "《猎场》到底好不好看？我们还是想通过以 11 月 8 日为界，看看人们短评人的情绪，是积极，还是消息。利用词云看看大家都说了什么，希望能大家就是否建议观看给出建议。", "同时建议在循环抓取的时候进行 sleep，例如：", "《猎场》热门短评内容和时间爬取了 22440 条评论，代码如下：", "样本数量：", "对热门短评基于原有 SnowNLP 进行积极和消极情感分类，读取每段评论并依次进行情感值分析（代码：", "），最后会计算出来一个 0-1 之间的值。", "当值大于 0.5 时代表句子的情感极性偏向积极，当分值小于 0.5 时，情感极性偏向消极，当然越偏向两边，情绪越偏激。", "2017-11-06 – 2017-11-08 分析：", "从上图情感分析（代码：", " ）来看，影评者还是还是非常积极的，对《猎场》的期望很高。", "从词云（代码：", " ）上来看：", "2017-11-09 – 2017-11-17 分析", "从上图情感分析（代码：", " ）来看，积极的情绪已经远远超过消极的情绪，还是受到大家的好评。", "从词云（代码：", " ）上来看，出现好看、剧情、期待、喜欢等词。", "词云的背景是胡歌，大家看出来了嘛？目前豆瓣的分数已经是 6.2 分，目前剧情过半，相信接下来会更精彩，个人认为分数会在 7.5 分以上。", "抛开豆瓣的推荐分数，通过的热门短评的情感和词云分析，是一部不错的现实剧，剧情犀利、深刻、启迪，很多人期待。如果您有时间，不妨看一下，或许能收获一些意想不到的东西。"], "art_url": "http://python.jobbole.com/88912/", "art_img_url": "http://wx4.sinaimg.cn/mw690/63918611gy1fls2ib5qbij20hs0a0gmf.jpg", "art_title": "差评近一半，用 Python 分析胡歌的《猎场》到底值不值得看？"}
{"create_time": "2017-11-24", "art_content": ["在", "中我们提到当决策树和装袋法(Bagging)和提升法(Boosting)结合后会成为更强大的算法，那么今天就介绍一种名叫随机森林(Random Forest)的算法，它是将决策树、装袋法以及随机特征选取结合后衍生出的一种增强型的树算法。", "它有如下特点：", "看到随机森林有这么多的优点，你是不是心动了呢？那么接下来和我一起来认识一下它吧！", "上文提到随机森林不是一种全新的算法，而是几种算法的强强联合。随机森林的构建一般有这么几个步骤：", "算法作者说OOB Error可以作为测试误差的无偏估计，也就是计算出OOB Error就可以得到测试误差，不用专门把数据专门拿出来一部分作为测试集。下面举例说明如何计算OOB Error，比如我们要在一个有7条数据的数据集上构建一个5棵树的随机森林，那么在步骤1的时候会出现下面这样一张表:", "表里的X代表没有选中，√代表选中。对于树1，数据1和7就是OOB，对于树2，数据2和5就是OOB，其他以此类推，那么数据1的预测值由树1和树4决定，数据2的预测值由树2-5来决定，以这样的方式计算出每个数据的预测值，进而得到误差值，即OOB Error。", "假设我们已经计算出了OOB Error，一个变量的重要性可以这么计算，将变量打散，然后重新计算打散后的OOB Error，取打散前后OOB Error差值的绝对值，越大代表这个变量越重要。变量重要性在实践过程中非常好用，比如在一个10000维度的数据集选出100个最重要的变量，即数据的降维。", "相似性由相似性矩阵体现，相似性矩阵是一个NxN的对称矩阵，它的计算方式如下，如果数据n和数据p同属于同一颗树的同一个叶子节点，那么相似性加1，即proximities[n,p]和proximities[p,n]均加1，最后除以树的数目进行标准化。", "有了相似性，也就可以计算离群点了。它基于这样的假设，如果一条数据和其他数据都不相似或者相似性很低，那么这条数据很可能是个离群点。这和人很类似阿，如果一个人不合群，那么他肯定是比较孤立的。不过在我实际操作的过程中，即使计算出了潜在的离群点，如何确定它真的是不是不是那么容易。", "具体的计算过程如下，定义类别为j的数据n的平均相似性为：", "得到非相似性：", "然后在各自的类别中标准化，得到最终的Dissimilarity，算法作者给出的经验值是如果一条数据的Dissimilarity>10，那么可能是一个潜在的离群点。", "对于缺失值，传统的方法就是数值变量取均值，分组变量取最多的那一类。而随机森林处理缺失值另有一套：先使用一个不太准确的初始值替换缺失值，然后计算数据间的相似性，数值变量取同一类别非缺失值的相似性加权平均；分组变量取频率最高的值，频率要经过相似性加权，然后重复这一过程4-6次。", "在决策树代码的基础上稍加改动就得到了随机森林，下面检验一下新算法的能力。", "1、在", "里我尝试使用花萼长度(Sepal.Length)和花萼宽度(Sepal.Width)这两个变量来预测鸢尾花的种类(Species)，这里用随机森林试一试。", "首先来看下不同数目的树对分类的影响，下图的分类边界(Decision Boundary)，使用的Node Size为1，特征数Feature Count也为1,", "可以看到，与决策树相比，随机森林对过拟合(Overfit)有着很强的抗性，且随着树的数目增多过拟合越来越少。但是，另一方面也要看到尽管对过拟合很强的抗性，还是可以看到过拟合的影子，即便我们已经用了1000棵树。所以，还是要为随机森林选择一个合适的Node Size,", "从上面的第一张图，可以看到Node Size从0～100增加时，OOB Error先降后增，且在Node Size为15时达到最低。从第二张图可以看到随机森林分类边界的变化过程，先是轻微的过拟合继而最合适的边界最后严重的欠拟合。第三张图是最合适的分类边界，尽管和决策树一样，预测的错误率都为0.2左右，但是和决策树的分类边界相比，随机森林的边界更平滑。", "2、", "为了和决策树作对比，我也用随机森林来预测下房价(price)，也是使用区域(area)、是否学区(school)、是否有地铁(subway)、总价(num)这四个变量，使用的参数为树的数目TS=100，特征数Feature Count=4，节点数目Node Size=5，得到的结果如下，", "袋外决策系数R2为0.7，使用模型预测所有房屋价格的决策系数为0.74，比决策树的0.7高了4个百分点，大家不要小看了这4个百分点，在机器学习中哪怕1个百分点都要付出很大的努力。况且，我在这里并没有使用交叉验证获取最佳的参数，只是凭经验选取。另外模型还给出了预测房价各个变量的重要性，可以看到决定房价最重要的就是房子所在的区。", "接下来是个分类问题，使用小区(region)、户型(zone)、面积(meters)、朝向(direction)、区域(con)、楼层(floor)、房龄(year)、学区(school)、地铁(subway)、税(tax)、总价(num)、单价(price)来预测区(area)。随机从29790中抽取了10000条数据构造100颗树的随机森林，构建一个100棵树的森林，OOB Error和变量重要性如下，", "OOB Error约为0.12，使用得到的随机森林模型预测29790条房源的区域，误差约为0.08，两者还是比较接近的。不出所料，片区(con)的重要性最高，另外房价(price)、是否学区(school)对房子区域的重要性也决非浪得虚名。", "上文提到，随机森林可以作为降维的工具，我从中选择前6个重要的变量重新构建一个随机森林，OOB Error和变量重要性如下，", "可以看到，使用6个变量的OOB Error与使用全部12个变量的OOB Error不相上下。", "下面看看有没有潜在的离群点，下图是每套房子的Dissimilarity，", "有两套房子的Dissimilarity>10，看看是什么房子，", "初步看来，这两套房子的总价(num)和价格(price)有点高，是不是这一点让它们鹤立鸡群呢？", "本文简单介绍了随机森林的特点及算法，并简单分析了iris和北京二手房两个数据集。本文只是抛砖引玉，其实随机森林的还有一些其他的特性，大家可以多多去发掘。", "参考资料："], "art_url": "http://python.jobbole.com/88884/", "art_img_url": "http://jbcdn2.b0.upaiyun.com/2017/11/1ed7c6d9074f6464e4702ca32251fc8a.png", "art_title": "疏而不漏：随机森林"}
{"create_time": "2017-11-28", "art_content": ["无论你是正在使用 Python 进行快速开发，还是在为 Python 桌面应用制作原生 UI ，或者是在优化现有的 Python 代码，以下这些 Python 项目都是应该使用的。", "Python 凭借其易用的特点，已经被工业界和学术界广泛采用。另一方面，Python 丰富的第三方项目——库、附加组件，和辅助的开发成果——使得 Python 语言的应用范围被不断扩大。", "其中一些项目，比如 PyInstaller 和 WxPython ，为那些制作桌面应用和终端应用的 Python 开发者提供了便利。其他的项目, 比如 PyPy , 则是用来给服务器端 Python 应用提供额外的动力。还有一些，像 PBR 、CFFI 和 MyPy , 适用于差不多所有五花八门的 Python 应用，无论在什么地方运行。", "如果你是一个 Python 开发者，所有这六个项目都值得你来熟悉一下。而且所有这些项目，在近几周都发布了新的主要版本。", "如果你需要更快的 Python 应用程序，最简单的实现的方法就是通过 PyPy ，Python 运行时与实时（JIT）编译器。与使用普通的 Python 对等程序相比，使用 PyPy 的 Python 应用程序的运行速度平均提升7.5倍。不幸的是，PyPy 与许多 Python 的明星框架并不是很好地兼容。", " 在解决这个问题上取得了重大进展。", "数据科学框架 NumPy 和 Pandas 现在运行在 PyPy 的 Python 2.7 兼容版本上。这些框架的大部分问题来源于 PyPy 与现有 C 代码的接口。为了解决这个问题，PyPy 5.9 对 CFFI 库（见下文）和 PyPy 的 Python C API 兼容性层进行了改进。", "\n此外，在 5.9 发布版本中，PyPy 的 JSON 解析器在处理多种 JSON 对象，尤其是那些重复使用的相同的词典键值时，明显更快。", "官方二进制文件包括 Windows、Mac OS 和 Linux 的不同 CPU 架构。请注意，为了兼容 Python 2.7 和 Python 3.5 ，存在不同的二进制文件，因此请确保你正在获取与你将要运行的脚本所匹配的版本。", "（CFFI）为 Python 应用程序与独立 C 库的交互提供了一种机制。虽然 Python 的 stock 版本，CPython，也拥有自己的库来完成此类功能，称为 ", " ，但对 Python 用户来说，比起 Ctypes ，CFFI 使得与 C 库的交互更容易、更简便。", "与 PyPy 一起更新的 ", " 增加了很小但很有用的改动。现在可以在即将发布的 Python 3.7 上使用betas了，在 Windows 上更好地支持外部错误处理，并支持 C 语言中更多的现代标准类型，例如 float/double _Complex 和 char16_t和char_32t 类型。最后两个也是最重要的，在 C 库中默认使用 Unicode 编码。", "CFFI 在 ", "，或通过 Python 的 pip 工具安装：pip install cffi 。源码和问题跟踪可以在 ", " 上找到。", "关于 Python 的最常见的问题之一是“如何从 Python 脚本中生成独立的可执行文件？” ", " 一直是对此最好的答案之一。", "PyInstaller 将 Python 应用程序打包到单目录或单文件的可执行文件中，捆绑任何所需的第三方库，并可与绝大多数常见的库和框架配合使用。", " 中最大的改进是对 Python 3.6 的支持，因为鉴于 Python 3.6 已经发布这确实是必要的", "PyInstaller 3.3 还包括一个更广泛兼容的引导加载程序，适用于 Windows 可执行文件，并扩展了对捆绑常见库（如 QT、GTK +、NumPy 和 Django ）的支持。", "PyInstaller 在不久之后可能添加的一个功能是交叉打包，例如，在 Windows 上创建 Mac 兼容的应用程序。你需要在要部署的同一平台上运行该 PyInstaller ，无论是 Windows、Mac 还是 Linux 。", "，也可通过 Python 的 pip 工具安装：pip install pyinstaller 。对于那些需要自己编译引导加载程序的人，", " 上找到，但对多数人而言是不需要这么做的。", "Setuptools 是用于打包 Python 项目的标准的 Python 问题子系统。管理特定项目的 Setuptools 可能会变得非常繁琐，特别是在自动生成需求、管理文档文件或编辑项目贡献者数据时。", ", Python Build Reasonableness 的缩写，是以一致的方式用于管理 Setuptools 包的库。它可以自动化许多 Setuptools 打包的设置，例如版本号、生成作者和 ChangeLog 文件，以及生成 Sphinx 风格的文档。PBR 最初是作为 OpenStack 项目的一部分开发的，但现在你所使用 PBR 中维护的内容与 OpenStack 已经没有任何联系了。", "，并且可以和 pip 一起安装，只需要输入 pip install pbr 即可。 源码可在 ", " 上下载。", "想要实现跨平台桌面应用程序的 Python 开发人员可以从多个工具包中进行选择。 ", "，是 ", " 库的一个封装，使用了其所支持主机平台的原生 UI 元素，包括 Windows、Mac、Linux 和其他类 Unix 操作系统。", "早期版本的 WxPython 被放弃了是由于其传统的设计决策，使其变得越来越慢，而且不太适合使用。为了解决这个问题，WxPython 的开发人员对 WxPython 的 4.0 分支做了重大改变。", "目标是允许开发人员更快地上手 WxPython ，并且使通过它创建的框架和应用程序更加高性能和易维护。然而，为了使用 WxPython 4.0 ，任何现有的使用 WxPython 项目都", "。", "WxPython 4.0 官方版本依然是 beta 版。它可以在 ", "，即通过 pip install wxpython 命令。在正式发布前它可能会更新数次，注意经常检查更新。", "那些想直接破解的人可以查看 ", "。请注意，WxPython 的 4.0 分支以 “Phoenix” 代号进行标记的，以使其与早期版本不同。", "Python 的动态性既是一种福音，也是一种烦恼，对于快速构建软件非常棒，但是当代码难以推理、测试和调试时，并不是很棒。", " 在编译时向 Python 添加静态类型检查，使 Python 程序更加一致和可维护，并且不会增加运行时开销。", " 添加了不同", "的支持，该协议是用于 Python 子类的目前实验性类型的功能。它还在仅用于包含特定类型的对象的字典中添加 “TypedDict” 类型，并且可以逐个对文件进行更严格的类型检查的选项。", "Mypy 可以在 ", "，并通过 pip install mypy 来安装。那些对 Mypy 实现感兴趣的人可以通过 ", " 检出源码。"], "art_url": "http://python.jobbole.com/88958/", "art_img_url": "http://jbcdn2.b0.upaiyun.com/2017/11/722f435a6aa288edb0916821e53d878d.png", "art_title": "Python 开发者的 6 个必备库"}
{"create_time": "2017-11-23", "art_content": ["如果你用Python语言做过任何的数据分析，那么可能会用到", ",一个由Wes McKinney写的奇妙的分析库。通过赋予Python数据帧以分析功能，Pandas已经有效地把Python和一些诸如R或者SAS这样比较成熟的分析工具置于相同的地位。", "不幸的是，在早期，Pandas因“慢”而声名狼藉。的确，Pandas代码不可能达到如完全优化的原始C语言代码的计算速度。然而，好消息是，对于大多数应用程序来说，写的好的Pandas代码已", "；Pandas强大的功能和友好的用户体验弥补了其速度的缺点。", "在这篇文章中，我们将回顾应用于Pandas DataFrame函数的几种方法的效率，从最慢到最快：", "1． 在用索引的DataFrame行上的Crude looping", "\n2． 用iterrows()循环", "\n3． 用 apply()循环", "\n4． Pandas Series矢量化", "\n5． NumPy数组矢量化", "对于我们的实例函数，将使用", "（半正矢）距离公式。函数取两点的经纬度，调整球面的曲率，计算它们之间的直线距离。这个函数看起来像这样：", "为了在真实数据上测试函数，我们用一个包含纽约所有酒店坐标的数据集，该数据来自", "。要计算每一个酒店和一个样本集坐标之间的距离（这恰好属于在纽约市名为", "的一个梦幻般的小商店）", "大家可以下载数据集，Jupyter notebook（是一个交互式笔记本，支持运行 40 多种编程语言）包含了用于这篇博客的函数，请点击", "下载。", "这篇文章基于我的PyCon访谈，大家可以在", "观看。", "首先，让我们快速回顾一下Pandas数据结构的基本原理。Pandas的基本结构有两种形式：", "和", "。一个DataFrame是一个二维", "标记轴，很多功能与R中的data.frame类似，可以将DataFrame理解为Series的容器。换句话说，一个DataFrame是一个有行和列的矩阵，列有列名标签，行有索引标签。在Pandas DataFrame中一个单独的列或者行是一个Pandas Series—一个带有轴标签的一维数组。", "几乎每一个与我合作过的Pandas初学者，都曾经试图通过一次一个的遍历DataFrame行去应用自定义函数。这种方法的优点是，它是Python对象之间交互的一致方式；例如，一种可以通过列表或数组循环的方式。反过来说，不利的一面是，在Pandas中，Crude loop是最慢的方法。与下面将要讨论的方法不同，Pandas中的Crude loop没有利用任何内置优化，通过比较，其效率极低（而且代码通常不那么具有可读性）", "例如，有人可能会写像下面这样的代码：", "为了了解执行上述函数所需要的时间，我们用", "命令。", "是一个“", "”命令，专用于", "（所有的魔法命令都以%标识开始，如果%命令只应用于一行，那么%%命令应用于整个Jupyter单元）。", "命令将多次运行一个函数，并打印出获得的运行时间的平均值和标准差。当然，通过", "命令获得的运行时间，运行该函数的每个系统都不尽相同。尽管如此，它可以提供一个有用的基准测试工具，用于比较同一系统和数据集上不同函数的运行时间。", "结果是：", "通过分析，crude looping函数运行了大约645ms,标准差是31ms。这似乎很快，但考虑到它仅需要处理大约1600行的代码，因此它实际上是很慢的。接下来看看如何改善这种不好的状况。", "如果循环是必须的，找一个更好的方式去遍历行，比如用", "方法。", "是一个生成器，遍历DataFrame的所有行并返回每一行的索引，除了包含行自身的对象。", " 是用Pandas DataFrame优化，尽管它是运行大多数标准函数最不高效的方式（稍后再谈），但相对于Crude looping，这是一个重大的改进。在我们的案例中，", "解决同一个问题，几乎比手动遍历行快四倍。", "一个比", "更好的选择是用 ", " 方法，它应用一个函数，沿着DataFrame某一个特定的轴线（意思就是行或列）。虽然", "也固有的通过行循环，但它通过采取一些内部优化比", "更高效，例如在Cython中使用迭代器。我们使用一个匿名的lambda函数，每一行都用Haversine函数，它允许指向每一行中的特定单元格作为函数的输入。为了指定Pandas是否应该将函数应用于行（", "）或列（", "），Lambda函数包含最终的", "参数。", "方法用", "方法替代后，大致可以将函数的运行时间减半。为了更深入地了解函数中的实际运行时间，可以运行一个", "（Jupyter中神奇的命令", "）", "结果如下：", "我们可以从这个信息中得到一些有用的见解。例如，进行三角计算的函数占了总运行时间的近一半。因此，如果想优化函数的各个组件，可以从这里入手。现在，特别值得注意的是每一行都被循环了1631次—apply（）遍历每一行的结果。如果可以减少重复的工作量，就可以降低整个运行时间。矢量化提供了一种更有效的替代方案。", "要了解如何可以减少函数所执行的迭代数量，就要记得Pandas的基本单位，DataFrame和Series，它们都基于数组。基本单元的固有结构转换成内置的设计用于对整个数组进行操作的Pandas函数，而不是按各个值的顺序（简称", "）。", "是对整个数组执行操作的过程。", "Pandas包含一个总体的矢量化函数集合，从数学运算到聚合和字符串函数（可用函数的扩展列表，查看", "）。对Pandas Series和DataFrame的操作进行内置优化。结果，使用矢量Pandas函数几乎总是会用自定义的循环实现类似的功能。", "到目前为止，我们仅传递标量给Haversine函数。所有的函数都应用在Haversine函数中，也可以在数组上操作。这使得距离矢量化函数的过程非常的简单：不是传递个别标量值的纬度和经度给它，而是把它传递给整个series（列）。这使得Pandas受益于可用于矢量函数的全套优化，特别是包括同时执行整个数组的所有计算。", "通过使用", "方法，要比用", "方法改进50倍的效率，通过矢量化函数则改进了", "方法100倍—除了改变输入类型，什么都不要做！", "看一眼后台，看看函数到底在做什么：", "注意，鉴于 apply() 执行函数1631次，矢量化版本仅执行一次，因为它同时应用于整个数组，这就是主要的时间节省来源。", "Pandas series矢量化可以完成日常计算优化的绝大多数需要。然而，如果速度是最高优先级，那么可以以NumPy Python库的形式调用援军。", "，将自己描述为一个“Python科学计算的基本包”，在后台执行优化操作，预编译C语言代码。跟Pandas一样，NumPy操作数组对象（简称ndarrays）；然而，它省去了Pandas series操作所带来的大量资源开销，如索引、数据类型检查等。因此，NumPy数组的操作可以明显快于pandas series的操作。", "当Pandas series提供的额外功能不是很关键的时候，NumPy数组可以用于替代Pandas series。例如，Haversine函数矢量化实现不使用索引的经度和纬度系列，因此没有那些索引，也不会导致函数中断。通过比较，我们所做的操作如DataFrame的连接，它需要按索引来引用值，可能需要坚持使用Pandas对象。", "仅仅是使用Pandas series 的", "的方法，把纬度和经度数组从Pandas series转换到NumPy数组。就像series矢量化一样，通过NumPy数组直接进入函数将可以让Pandas对整个矢量应用函数。", "NumPy数组操作运行取得了又一个四倍的改善。总之，通过looping改进了运行时间超过半秒，通过NumPy矢量化，运行时间改进到了三分之一毫秒级！", "下面的表格总结了相关结果。用NumPy数组矢量化将会带来最快的运行时间，相对于Pandas series矢量化的效果而言，这是一个很小的改进，但对比最快的looping版本，NumPy数组矢量化带来了56倍的改进。", "这给我们带来了一些关于优化Pandas代码的基本结论：", "当然，以上并不是Pandas所有可能优化的全面清单。更爱冒险的用户或许可以考虑进一步用", "改写函数，或者尝试优化函数的各个组件。然而，这些话题超出了这篇文章的范围。", "关键的是，在开始一次宏大的优化冒险之前，要确保正在优化的函数实际上是你希望在长期运行中使用的函数。引用XKCD不朽的名言：“", "”。"], "art_url": "http://python.jobbole.com/88915/", "art_img_url": "http://jbcdn2.b0.upaiyun.com/2017/11/0555f99614ede2b6f00917d386e27788.png", "art_title": "Pandas初学者代码优化指南"}
{"create_time": "2017-12-01", "art_content": ["我最近在涉及大量数据处理的项目中频繁使用 sqlite3。我最初的尝试根本不涉及任何数据库，所有的数据都将保存在内存中，包括字典查找、迭代和条件等查询。这很好，但可以放入内存的只有那么多，并且将数据从磁盘重新生成或加载到内存是一个繁琐又耗时的过程。", "我决定试一试sqlite3。因为只需打开与数据库的连接，这样可以增加可处理的数据量，并将应用程序的加载时间减少到零。此外，我可以通过 SQL 查询替换很多Python逻辑语句。", "我想分享一些关于这次经历的心得和发现。", "如果你需要在数据库中一次性插入很多行，那么你真不应该使用 execute。sqlite3 模块提供了批量插入的方式：executemany。", "而不是像这样做：", "你可以利用这个事实，即 executemany 接受元组的生成器作为参数：", "这不仅更简洁，而且更高效。实际上，sqlite3 在幕后利用 executemany 实现 execute，但后者插入一行而不是多行。", "我写了一个小的基准测试，将一百万行插入空表（数据库在内存中）：", "一开始我经常搞混的事情就是，光标管理。在线示例和文档中通常如下：", "但大多数情况下，你根本不需要光标，你可以直接使用连接对象（", "末尾会提到）。", "\n像execute和executemany类似的操作可以直接在连接上调用。以下是一个证明此事的示例：", "你可能经常会看到使用fetchone或fetchall来处理SELECT查询结果的示例。但是我发现处理这些结果的最自然的方式是直接在光标上迭代：", "这样一来，只要你得到足够的结果，你就可以终止查询，并且不会引起资源浪费。当然，如果事先知道你需要多少结果，可以改用LIMIT SQL语句，但Python生成器是非常方便的，可以让你将数据生成与数据消耗分离。", "即使在处理SQL事务的中间，也会发生讨厌的事情。为了避免手动处理回滚或提交，你可以简单地使用连接对象作为上下文管理器。 在以下示例中，我们创建了一个表，并错误地插入了重复的值：", "…当它真的有用时", "在你的程序中有几个 pragma 可用于调整 sqlite3 的行为。特别地，其中一个可以改善性能的是synchronous：", "你应该知道这可能是危险的。如果应用程序在事务中间意外崩溃，数据库可能会处于不一致的状态。所以请小心使用！ 但是如果你要更快地插入很多行，那么这可能是一个选择。", "假设你需要在数据库上创建几个索引，而你需要在插入很多行的同时创建索引。把索引的创建推迟到所有行的插入之后可以导致实质性的性能改善。", "使用 Python 字符串操作将值包含到查询中是很方便的。但是这样做非常不安全，而 sqlite3 给你提供了更好的方法来做到这一点：", "此外，使用Python％s（或格式或格式的字符串常量）的字符串插值对于executemany来说并不是总是可行。所以在此尝试没有什么真正意义！", "请记住，这些小技巧可能会（也可能不会）给你带来好处，具体取决于特定的用例。你应该永远自己去尝试，决定是否值得这么做。"], "art_url": "http://python.jobbole.com/88954/", "art_img_url": "http://jbcdn2.b0.upaiyun.com/2015/02/edecceebedd0d04aa17bccba430ddcaf.jpg", "art_title": "让 Python 更加充分的使用 Sqlite3"}
{"create_time": "2017-11-22", "art_content": ["尽管一些人认为区块链是一个等待问题的解决方案，但毫无疑问，这种新技术是计算机的奇迹。但是，区块链到底是什么呢?", "在更一般的术语中，它是一个公共数据库，新数据存储在一个名为块的容器中，并被添加到一个不可变链（后来的区块链）中添加了过去的数据。在比特币和其他加密货币的情况下，这些数据是一组交易记录。当然，数据可以是任何类型的。", "区块链技术已经催生了新的、完全数字化的货币，如比特币和莱特币，这些货币并不是由中央政府发行或管理的。因此为那些认为今天的银行系统是骗局或终将失败的人带来了新的自由。区块链所包含的以太坊技术对分布式计算进行了变革创新，它引入了一些有趣的概念，比如", "。", "在本文中，我将用不到50行的Python2代码来做一个简单的区块链。我称它为SnakeCoin。", "首先将定义块将是什么样子。在区块链中，每个块都存储一个时间戳和一个索引。在SnakeCoin中，需要把两者都存储起来。为了确保整个区块链的完整性，每个块都有一个自动识别散列。与比特币一样，每个块的散列将是块索引、时间戳、数据和前块哈希的加密哈希。数据可以是你想要的任何东西。", "这一步后有块结构，但现在是创建区块链，所以需要向实际的链中添加块。如前所述，每个块都需要上一个块的信息。但是按照这个说法就有一个问题，区块链的第一个区块是如何到达那里的呢？不得不说，第一个块，或者说是起源块，它是一个特殊的块。在很多情况下，它是手动添加的，或者有独特的逻辑允许添加。", "下面将创建一个函数简单地返回一个起源块以便产生第一个区块。这个块是索引0，它具有任意的数据值和“前一个哈希”参数中的任意值。", "现在已经创建好了起源块，接下来需要一个函数，以便在区块链中生成后续的块。这个函数将把链中的前一个块作为参数，创建要生成的块的数据，并使用适当的数据返回新块。当新的块哈希信息来自前面的块时，区块链的完整性会随着每个新块而增加。如果不这样做，外部组织就更容易“改变过去”，用全新的方式取代已有的链条。这一系列的散列可以作为加密的证据，有助于确保一旦将块添加到区块链，它就不能被替换或删除。", "大部分的工作已经完成，现在可以创建区块链了。在这次的示例中，区块链本身是一个简单的Python列表。列表的第一个元素是起源块。当然，还需要添加后续的块，因为SnakeCoin是最小的区块链，这里只添加20个新的块。可以用for循环来生成新块。", "下面来测试一下目前产生的区块链。", "看到了吧，这就是区块链。如果希望在控制台中查看更多信息，可以", "并打印每个块的时间戳或数据。", "这就是SnakeCoin要提供的所有东西。为了使SnakeCoin规模达到今天生产区块链的规模，必须添加更多的功能，比如服务器层，以跟踪多台机器上的链变化，以及在给定的时间段内限制添加的块数量的", "。", "如果想了解更多的技术信息，可以在", "查看原始的比特币白皮书。"], "art_url": "http://python.jobbole.com/88907/", "art_img_url": "http://jbcdn2.b0.upaiyun.com/2017/11/8cdceb83f2089cfe344252cbd73bb70f.png", "art_title": "用不到 50 行的 Python 代码构建最小的区块链"}
{"create_time": "2017-11-25", "art_content": ["搜索是大数据领域里常见的需求。Splunk和ELK分别是该领域在非开源和开源领域里的领导者。本文利用很少的Python代码实现了一个基本的数据搜索功能，试图让大家理解大数据搜索的基本原理。", "第一步我们先要实现一个", "。", "布隆过滤器是大数据领域的一个常见算法，它的目的是过滤掉那些不是目标的元素。也就是说如果一个要搜索的词并不存在与我的数据中，那么它可以以很快的速度返回目标不存在。", "让我们看看以下布隆过滤器的代码：", "看到这里，大家应该可以看出，如果布隆过滤器返回False，那么数据一定是没有索引过的，然而如果返回True，那也不能说数据一定就已经被索引过。在搜索过程中使用布隆过滤器可以使得很多没有命中的搜索提前返回来提高效率。", "我们看看这段 code是如何运行的：", "结果：", "首先创建了一个容量为10的的布隆过滤器", "然后分别加入 ‘dog’，‘fish’，‘cat’三个对象，这时的布隆过滤器的内容如下：", "然后加入‘bird’对象，布隆过滤器的内容并没有改变，因为‘bird’和‘fish’恰好拥有相同的哈希。", "最后我们检查一堆对象（’dog’, ‘fish’, ‘cat’, ‘bird’, ‘duck’, ’emu’）是不是已经被索引了。结果发现‘duck’返回True，2而‘emu’返回False。因为‘duck’的哈希恰好和‘dog’是一样的。", " ", "下面一步我们要实现分词。 分词的目的是要把我们的文本数据分割成可搜索的最小单元，也就是词。这里我们主要针对英语，因为中文的分词涉及到自然语言处理，比较复杂，而英文基本只要用标点符号就好了。", "下面我们看看分词的代码：", "主要分割", "主要分割使用空格来分词，实际的分词逻辑中，还会有其它的分隔符。例如Splunk的缺省分割符包括以下这些，用户也可以定义自己的分割符。", "] < >( ) { } | ! ; , ‘ ” * \\n \\r \\s \\t & ? + %21 %26 %2526 %3B %7C %20 %2B %3D — %2520 %5D %5B %3A %0A %2C %28 %29", "次要分割", "次要分割和主要分割的逻辑类似，只是还会把从开始部分到当前分割的结果加入。例如“1.2.3.4”的次要分割会有1，2，3，4，1.2，1.2.3", "分词的逻辑就是对文本先进行主要分割，对每一个主要分割在进行次要分割。然后把所有分出来的词返回。", "我们看看这段 code是如何运行的：", " ", "好了，有个分词和布隆过滤器这两个利器的支撑后，我们就可以来实现搜索的功能了。", "上代码：", "我们运行下看看把：", "是不是很赞！", " ", "更进一步，在搜索过程中，我们想用And和Or来实现更复杂的搜索逻辑。", "上代码：", "利用Python集合的intersection和union操作，可以很方便的支持And（求交集）和Or（求合集）的操作。", "运行结果如下：", " ", "以上的代码只是为了说明大数据搜索的基本原理，包括布隆过滤器，分词和倒排表。如果大家真的想要利用这代码来实现真正的搜索功能，还差的太远。所有的内容来自于Splunk Conf2017。大家如果有兴趣可以去看网上的视频。"], "art_url": "http://python.jobbole.com/88921/", "art_img_url": "http://jbcdn2.b0.upaiyun.com/2015/02/591d8b55a524f825dd29a22b8df70000.jpg", "art_title": "用 Python 实现一个大数据搜索引擎"}
{"create_time": "2017-11-19", "art_content": ["实际项目中，pythoner更加关注的是Python的性能问题，之前也写过一篇文章《", "》介绍Python性能优化的一些方法。而本文，关注的是Python的内存优化，一般说来，如果不发生内存泄露，运行在服务端的Python代码不用太关心内存，但是如果运行在客户端（比如移动平台上），那还是有优化的必要。具体而言，本文主要针对的Cpython，而且不涉及C扩展。", "我们知道，Python使用引用技术和垃圾回收来管理内存，底层也有各种类型的内存池，那我们怎么得知一段代码使用的内存情况呢？工欲善其事必先利其器，直接看windows下的任务管理器或者linux下的top肯定是不准的。", "对于基本类型，可以通过sys.getsizeof()来查看对象占用的内存大小。以下是在64位Linux下的一些结果：", "可以看到，即使是一个int类型(1)也需要占用24个字节，远远高于C语言中int的范围。因为Python中一切都是对象，int也不例外（事实上是PyIntObject），除了真正存储的数值，还需要保存引用计数信息、类型信息，更具体的可以参见《Python源码剖析》。", "而对于更复杂的组合类型，复杂的代码，使用getsizeof来查看就不准确了，因为在Python中变量仅仅指向一个对象，这个时候就需要更高级的工具，比如", "，", "，", "，", "。在这里重点介绍pytracemalloc。", "在Python3.4中，已经支持了pytracemalloc，如果使用python2.7版本，则需要对源码打补丁，然后重新编译。pytracemalloc在", "中提出，主要有以下几个特点：", "简单来说，pytracemalloc hook住了python申请和释放内存的接口，从而能够追踪对象的分配和回收情况。对内存分配的统计数据可以精确到每个文件、每一行代码，也可以按照调用栈做聚合分析。而且还支持快照（snapshot）功能，比较两个快照之间的差异可以发现潜在的内存泄露。", "下面通过一个例子来简单介绍pytracemalloc的用法和接口，关于更详细用法和API，可以参考这份详尽的", "或者pytracemalloc的作者在pycon上的", "。", "在上面的代码中，用到了pytracemalloc几个核心的API：", "pytracemalloc的一大好处就是可以随时启停，start函数即开始追踪内存分配，相应的stop会停止追踪。start函数有一个参数，nframes : 内存分配时记录的栈的深度，这个值越大，pytracemalloc本身消耗的内存越多，在计算cumulative数据的时候有用。", "返回值是拥有两个元素的tuple，第一个元素是当前分配的内存，第二个元素是自内存追踪启动以来的内存峰值。", "返回当前内存分配快照，返回值是Snapshot对象，该对象可以按照单个文件、单行、单个调用栈统计内存分配情况", "运行环境：windows 64位python3.4", "如果将第36行的“lineno“改成“filename”，那么结果如下", "有了Profile结果之后，可以看出来在哪个文件中有大量的内存分配。与性能优化相同，造成瓶颈的有两种情况：单个对象占用了大量的内存；同时大量存在的小对象。对于前者，优化的手段并不多，惰性初始化属性可能有一些帮助；而对于后者，当同样类型的对象大量存在时，可以使用slots进行优化。", "默认情况下，自定义的对象都使用dict来存储属性（通过obj.__dict__查看），而python中的dict大小一般比实际存储的元素个数要大（以此降低hash冲突概率），因此会浪费一定的空间。在新式类中使用__slots__，就是告诉Python虚拟机，这种类型的对象只会用到这些属性，因此虚拟机预留足够的空间就行了，如果声明了__slots__，那么对象就不会再有__dict__属性。", "使用slots到底能带来多少内存优化呢，首先看看", "，对于一个只有三个属性的Image类，使用__slots__之后内存从25.5G下降到16.2G，节省了9G的空间！", "到底能省多少，取决于类自身有多少属性、属性的类型，以及同时存在多少个类的实例。下面通过一段简单代码测试一下：", "上面的代码，主要是在每个实例的属性数目、并发存在的实例数目两个维度进行测试，并没有测试不同的属性类型。结果如下表：", "百分比为内存优化百分比，计算公式为(b – a) / b， 其中b为没有使用__slots__时分配的内存， a为使用了__slots__时分配的内存。", "关于__slots__，Python文档有非常详尽的介绍，这里只强调几点注意事项", "：基类和子类都必须__slots__，即使基类或者子类没有属性", "从上面的示例可以看到，子类的对象还是有__dict__属性，原因就在于基类没有声明__slots__。因此，可以通过看子类的实例有没有__dict__属性来判断slots的使用是否正确", "：子类会继承基类的__slots__", "更准确的说，如果访问属性的时候没有在子类的__slots__找到，会继续在基类的__slots__查找，因为Python使用descriptor在类这个层级实现__slots__的，具体可以参见《", "》一文", "在大型工程中，怎么排查有哪些大量存在的对象呢，毕竟同一个类型存在的对象越多，优化越有效果。除了直接看代码，最好使的就是使用objgraph.py的show_most_common_types(N)函数，该函数返回Python gc管理的所有对象中，数目前N多的对象，在排除掉python builtin对象之后，剩下的就是可优化的对象。比如在最上面的代码中：在最后加上这么两句：", "输出如下：", "前面介绍slots的时候，就提到Python自定义的对象中通过dict来管理属性。这种机制极大的提高了Python的灵活性 — 可以随时给对象增加属性，但是其实现机制也带来了内存上的浪费。不管是python源码，还是Python程序，都大量使用了dict，因此这部分内存浪费不容小视。", "python中的dict使用的是散列表（类似C++中的std::unordered_map），当计算出的hash值冲突的时候，采用开放地址法解决冲突（另一种常见的冲突解决算法是链表法）。为了降低冲突概率，当装填因子（实际存储的元素与散列表长度的比值）超过2/3的时候就会对散列表进行扩容，因此散列表中一定会存在一些未使用的槽。", "下面简单看看PyDictObject的数据结构（python2.7.3 dictobject.h）", "从定义可以看出，除了固定的部分（几个Py_ssize_t），PyDictObject中主要是PyDictEntry对象，PyDictEntrty包含一个Py_ssize_t（int）和两个指针。上面源码中的注释（第26行）指出，当dict的元素比较少时，ma_table指向ma_smalltable，当元素增多时，ma_table会指向新申请的空间。ma_smalltable的作用在于Python（不管是源码还是代码）都大量使用dict，一般来说，存储的元素也不会太多，因此Python就先开辟好PyDict_MINSIZE(默认为8)个空间。", "为什么说PyDictObject存在浪费呢，PyDictEntry在32位下也有12个字节，那么即使在ma_smalltable（ma_table）中大量的位置没有被使用时，也要占用这么多字节。用", "中的例子：", "假设有这么一个dict：", "在Python源码中的视图就是这样的：", "然而，完全可以这么存储：", "indices的作用类似ma_smalltable，但只存储一个数组的索引值，数组只存储实际存在的元素（PyDictEntry），当dict中的元素越稀疏，相比上一种存储方式使用的内存越少。而且，这种实现， dict就是有序的（按插入时间排序）", "这就是python3.6中新的dict实现，Compact dict! Stackoverflow上也有相关", "。", "本文中介绍了Python内存优化的Profile工具，最有效的优化方法：使用slots，也介绍了在python3.6中新的dict实现。", "当然，还有一些良好的编码习惯。比如尽量使用immutable而不是mutable对象：使用tuple而不是list，使用frozenset而不是set；另外，就是尽量使用迭代器，比如python2.7中，使用xrange而不是range，dict的iterxx版本。"], "art_url": "http://python.jobbole.com/88896/", "art_img_url": "http://jbcdn2.b0.upaiyun.com/2017/11/f3b43c55d59635277f981c85325df4a5.png", "art_title": "Python 内存优化"}
{"create_time": "2017-11-19", "art_content": ["本文有些零碎，总题来说，包括两个问题：（1）可变对象（最常见的是list dict）被意外修改的问题，（2）对参数（parameter）的检查问题。这两个问题，本质都是因为动态语言（动态类型语言）的特性造成了，动态语言的好处就不细说了，本文是要讨论因为动态－－这种灵活性带来的一些问题。", "什么是动态语言（", "）呢，是相对于静态语言而言，将很多静态语言编译（compilation）时期所做的事情推迟到运行时，在运行时修改代码的行为，比如添加新的对象和函数，修改既有代码的功能，改变类型。绝大多数动态语言都是动态类型（Dynamic Typed），所谓动态类型，是在运行时确定数据类型，变量使用之前不需要类型声明，通常变量的类型是被赋值的那个值的类型。Python就是属于典型的动态语言。", "动态语言的魅力在于让开发人员更好的关注需要解决的问题本身，而不是冗杂的语言规范，也不用干啥都得写个类。运行时改变代码的行为也是非常有用，比如python的热更新，可以做到不关服务器就替换代码的逻辑，而静态语言如C++就很难做到这一点。笔者使用得最多的就是C++和Python，C++中的一些复杂的点，比如模板（泛型编程）、设计模式（比如template method），在Python中使用起来非常自然。我也看到过有一些文章指出，设计模式往往是特定静态语言的补丁 — 为了弥补语言的缺陷或者限制。", "以笔者的知识水平，远远不足以评价动态语言与静态语言的优劣。本文也只是记录在我使用Python这门动态语言的时候，由于语言的灵活性，由于动态类型，踩过的坑，一点思考，以及困惑。", "这个是在线上环境出现过的一个BUG", "事后说起来很简单，服务端数据（放在dict里面的）被意外修改了，但查证的时候也花了许多时间，伪代码如下：", "上述的代码很简单，dct是一个dict，极大概率会调用一个不用修改dct的子函数，极小概率出会调用到可能修改dct的子函数。问题就在于，调用routine函数的参数是服务端全局变量，理论上是不能被修改的。当然，上述的代码简单到一眼就能看出问题，但在实际环境中，调用链有七八层，而且，在routine这个函数的doc里面，声明不会修改dct，该函数本身确实没有修改dct，但调用的子函数或者子函数的子函数没有遵守这个约定。", "本小节解释上面的代码为什么会出问题，简单来说两点：dict是mutable对象； dict实例作为参数传入函数，然后被函数修改了。", "Python中一切都是对象(evething is object)，不管是int str dict 还是类。比如 a =5， 5是一个整数类型的对象（实例）；那么a是什么，a是5这个对象吗? 不是的，a只是一个名字，这个名字暂时指向（绑定、映射）到5这个对象。b = a 是什么意思呢， 是b指向a指向的对象，即a， b都指向整数5这个对象", "那么什么是mutable 什么是immutable呢，mutable是说这个对象是可以修改的，immutable是说这个对象是不可修改的(废话)。还是看Python官方怎么说的吧", "承接上面的例子（a = 5），int类型就是immutable，你可能说不对啊，比如对a赋值， a=6， 现在a不是变成6了吗？是的，a现在”变成”6了，但本质是a指向了6这个对象 — a不再指向5了", "检验对象的唯一标准是id，id函数返回对象的地址，每个对象在都有唯一的地址。看下面两个例子就知道了", "或者这么说，对于非可变对象，在对象的生命周期内，没有办法改变对象所在内存地址上的值。", "python中，不可变对象包括：int, long, float, bool, str, tuple, frozenset；而其他的dict list 自定义的对象等属于可变对象。", "： str也是不可变对象，这也是为什么在多个字符串连接操作的时候，推荐使用join而不是+", "而且python没有机制，让一个可变对象不可被修改（此处类比的是C++中的const）", "那在python中，调用函数时的参数传递是什么意思呢，是传值、传引用？事实上都不正确，我不清楚有没有专业而统一的说法，但简单理解，就是形参（parameter）和实参（argument）都指向同一个对象，仅此而已。来看一下面的代码：", "运行结果：", "可以看到，刚进入子函数double的时候，a，v指向的同一个对象（相同的id）。对于test int的例子，v因为v*=2，指向了另外一个对象，但对实参a是没有任何影响的。对于testlst的时候，v*=2是通过v修改了v指向的对象（也是a指向的对象），因此函数调用完之后，a指向的对象内容发生了变化。", "为了防止传入到子函数中的可变对象被修改，最简单的就是使用copy模块拷贝一份数据。具体来说，包括copy.copy, copy.deepcopy, 前者是浅拷贝，后者是深拷贝。二者的区别在于：", "简单来说，深拷贝会递归拷贝，遍历任何compound object然后拷贝，例如：", "从例子可以看出浅拷贝的局限性，Python中，对象的基本构造也是浅拷贝，例如", "正是由于浅拷贝与深拷贝本质上的区别，二者性能代价差异非常之大，即使对于被拷贝的对象来说毫无差异：", "运行结果：", "在上面的示例中，dct这个dict的values都是int类型，immutable对象，因为无论浅拷贝 深拷贝效果都是一样的，但是耗时差异巨大。如果在dct中存在自定义的对象，差异会更大", "那么为了安全起见，应该使用深拷贝；为了性能，应该使用浅拷贝。如果compound object包含的元素都是immutable，那么浅拷贝既安全又高效，but，对于python这种灵活性极强的语言，很可能某天某人就加入了一个mutable元素。", "好的API应该是", "。API应该提供一种契约，约定如果使用者按照特定的方式调用，那么API就能实现预期的效果。", "在静态语言如C++中，函数签名就是最好的契约。", "在C++中，参数传递大约有三种形式，传值、传指针、传引用（这里不考虑右值引用）。指针和引用虽然表现形式上差异，但效果上是差不多的，因此这里主要考虑传值和传引用。比如下面四个函数签名：", "对于第1、2个函数，对于调用者来说都是一样的，因为都会进行拷贝（深拷贝），无论func函数内部怎么操作，都不会影响到实参。二者的区别在于函数中能否对a进行修改，比如能否写 a *＝ 2。", "第3个函数，非const引用，任何对a的修改都会影响到实参。调用者看到这个API就知道预期的行为：函数会改变实参的值。", "第4个函数，const引用，函数承诺绝对不会修改实参，因此调用者可以放心大胆的传引用，无需拷贝。", "从上面几个API，可以看到，通过函数签名，调用者就能知道函数调用对传入的参数有没有影响。", "python是动态类型检查，除了运行时，没法做参数做任何检查。有人说，那就通过python doc或者变量名来实现契约吧，比如：", "但是人是靠不住的，也是不可靠的，也许在这个函数的子函数（子函数的子函数，。。。）就会修改这个dict。怎么办，对可变类型强制copy（deepcopy），但拷贝又非常耗时。。。", "上一节说明没有签名 对 函数调用者是多么不爽，而本章节则说明没有签名对函数提供者有多么不爽。没有类型检查真的蛋疼，我也遇到过有人为了方便，给一个约定是int类型的形参传入了一个int的list，而可怕的是代码不报错，只是表现不正常。", "来看一个例子：", "上述的代码很糟糕，根本没法“望名知意”，也看不出有关形参 arg的任何信息。但事实上这样的代码是存在的，而且还有比这更严重的，比如挂羊头卖狗肉。", "这里有一个问题，函数期望arg是某种类型，是否应该写代码判断呢，比如：isinstance(arg, str)。因为没有编译器静态来做参数检查，那么要不要检查，如何检查就完全是函数提供者的事情。如果检查，那么影响性能，也容易违背python的灵活性 — duck typing； 不检查，又容易被误用。", "但在这里，考虑的是另一个问题，看代码的第二行：", "。python中，几乎是一切对象都可以当作布尔表达式求值，即这里的arg可以是一切python对象，可以是bool、int、dict、list以及任何自定义对象。不同的类型为“真”的条件不一样，比如数值类型(int float)非0即为真；序列类型（str、list、dict）非空即为真；而对于自定义对象，在python2.7种则是看是否定义了__nonzero__ 、__len__，如果这两个函数都没有定义，那么实例的布尔求值一定返回真。", "在", "，由以下关于对序列布尔求值的规范：", "在", "中也有一节专门关于", "，指出“", "”。 对于序列，推荐的判断方法与pep8相同，另外还由两点比较有意思：", "第二点我个人很赞同；但第一点就觉得很别扭，因为这样的语句一点不直观，难以表达其真实目的。", "在", "中，指出：", "这句话简单但实用！代码是写给人读的，清晰的表达代码的意图比什么都重要。也许有的人觉得代码写得复杂隐晦就显得牛逼，比如python中嵌套几层的list comprehension，且不知这样害人又害己。", "回到布尔表达式求值这个问题，我觉得很多时候直接使用if arg：这种形式都不是好主意，因为不直观而且容易出错。比如参数是int类型的情况，", "很难说当age=0时是不是一个合理的输入，上面的代码对None、0一视同仁，看代码的人也搞不清传入0是否正确。", "另外一个具有争议性的例子就是对序列进行布尔求值，推荐的都是直接使用if seq: 的形式，但这种形式违背了”Explicit is better than implicit.“，因为这样写根本无法区分None和空序列，而这二者往往是由区别的，很多时候，空序列是一个合理的输入，而None不是。这个问题，", "上也有相关的讨论“如何检查列表为空”，诚然，如果写成 seq == [] 是不那么好的代码， 因为不那么灵活 — 如果seq是tuple类型代码就不能工作了。python语言是典型的duck typing，不管你传入什么类型，只要具备相应的函数，那么代码就可以工作，但是否正确地工作就完完全全取决于使用者。个人觉得存在宽泛的约束比较好，比如Python中的ABC（abstract base class）, 既满足了灵活性需求，后能做一些规范检查。", "以上两个问题，是我使用Python语言以来遇到的诸多问题之二，也是我在同一个地方跌倒过两次的问题。Python语言以开发效率见长，但是我觉得需要良好的规范才能保证在大型线上项目中使用。而且，我也倾向于假设：人是不可靠的，不会永远遵守拟定的规范，不会每次修改代码之后更新docstring …", "因此，为了保证代码的可持续发展，需要做到以下几点", "代码规范最好在项目启动时就应该拟定好，可以参照PEP8和google python styleguild。很多时候风格没有优劣之说，但是保证项目内的一致性很重要。并保持定期review、对新人review！", "只要能静态发现的bug不要放到线上，比如对参数、返回值的检查，在python3.x中可以使用注解（Function Annotations），python2.x也可以自行封装decorator来做检查。对代码行为，既可以使用Coverity这种高大上的商业软件，或者王垠大神的Pysonar2，也可以使用ast编写简单的检查代码。", "单元测试的重要性想必大家都知道，在python中出了官方自带的doctest、unittest，还有许多更强大的框架，比如nose、mock。", "对于python这种动态语言，出了执行代码，几乎没有其他比较好的检查代码错误的手段，所以覆盖率测试是非常重要的。可以使用python原生的sys.settrace、sys.gettrace，也可以使用coverage等跟更高级的工具。", "虽然我已经写了几年Python了，但是在Python使用规范上还是很欠缺。我也不知道在其他公司、项目中，是如何使用好Python的，如何扬长避短的。欢迎pythoner留言指导！"], "art_url": "http://python.jobbole.com/88901/", "art_img_url": "http://jbcdn2.b0.upaiyun.com/2015/02/edecceebedd0d04aa17bccba430ddcaf.jpg", "art_title": "动态语言的灵活性是把双刃剑 －－ 以 Python 语言为例"}
